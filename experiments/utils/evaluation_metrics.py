"""
äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°æŒ‡æ ‡å·¥å…·
ä¸“é—¨é’ˆå¯¹é‡‘èé¢„æµ‹çš„è¯„ä¼°æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨æ–¹å‘æ€§é¢„æµ‹å‡†ç¡®æ€§
"""

from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    precision_recall_fscore_support,
)


def calculate_directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    è®¡ç®—æ–¹å‘å‡†ç¡®ç‡ï¼ˆåªè€ƒè™‘æ¶¨è·Œï¼Œå¿½ç•¥æŒå¹³ï¼‰
    
    Args:
        y_true: çœŸå®æ ‡ç­¾ [0=æ¶¨, 1=è·Œ, 2=æŒå¹³]
        y_pred: é¢„æµ‹æ ‡ç­¾ [0=æ¶¨, 1=è·Œ, 2=æŒå¹³]
        
    Returns:
        æ–¹å‘å‡†ç¡®ç‡ (0-1)
    """
    # è¿‡æ»¤å‡ºæ¶¨è·Œæ ·æœ¬ï¼ˆéæŒå¹³ï¼‰
    non_flat_mask = (y_true != 2) & (y_pred != 2)
    
    if non_flat_mask.sum() == 0:
        return 0.0
    
    y_true_filtered = y_true[non_flat_mask]
    y_pred_filtered = y_pred[non_flat_mask]
    
    return accuracy_score(y_true_filtered, y_pred_filtered)


def calculate_catastrophic_error_rate(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """
    è®¡ç®—ç¾éš¾æ€§é”™è¯¯ç‡ï¼ˆæ–¹å‘å®Œå…¨ç›¸åçš„é¢„æµ‹ï¼‰
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        
    Returns:
        é”™è¯¯ç‡ç»Ÿè®¡å­—å…¸
    """
    total_samples = len(y_true)
    
    # æ–¹å‘æ€§é”™è¯¯
    up_to_down = ((y_pred == 0) & (y_true == 1)).sum()  # é¢„æµ‹æ¶¨å®é™…è·Œ
    down_to_up = ((y_pred == 1) & (y_true == 0)).sum()  # é¢„æµ‹è·Œå®é™…æ¶¨
    
    catastrophic_errors = up_to_down + down_to_up
    catastrophic_rate = catastrophic_errors / total_samples
    
    return {
        'catastrophic_error_rate': catastrophic_rate,
        'up_to_down_errors': up_to_down,
        'down_to_up_errors': down_to_up,
        'total_catastrophic': catastrophic_errors,
        'total_samples': total_samples
    }


def calculate_trading_value_score(y_true: np.ndarray, y_pred: np.ndarray, 
                                error_weights: Dict[str, float]) -> Dict[str, float]:
    """
    è®¡ç®—äº¤æ˜“ä»·å€¼è¯„åˆ†
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾  
        error_weights: é”™è¯¯æƒé‡é…ç½®
        
    Returns:
        è¯„åˆ†ç»Ÿè®¡
    """
    total_score = 0.0
    error_counts = {
        'correct': 0,
        'up_to_down': 0,
        'down_to_up': 0,
        'up_to_flat': 0,
        'down_to_flat': 0,
        'flat_to_up': 0,
        'flat_to_down': 0
    }
    
    for i in range(len(y_true)):
        true_label, pred_label = y_true[i], y_pred[i]
        
        if true_label == pred_label:
            # é¢„æµ‹æ­£ç¡®
            total_score += error_weights['correct_prediction']
            error_counts['correct'] += 1
        elif true_label == 0 and pred_label == 1:
            # é¢„æµ‹æ¶¨å®é™…è·Œ
            total_score += error_weights['up_to_down_error']
            error_counts['up_to_down'] += 1
        elif true_label == 1 and pred_label == 0:
            # é¢„æµ‹è·Œå®é™…æ¶¨
            total_score += error_weights['down_to_up_error']
            error_counts['down_to_up'] += 1
        elif true_label == 0 and pred_label == 2:
            # é¢„æµ‹æ¶¨å®é™…å¹³
            total_score += error_weights['up_to_flat_error']
            error_counts['up_to_flat'] += 1
        elif true_label == 1 and pred_label == 2:
            # é¢„æµ‹è·Œå®é™…å¹³
            total_score += error_weights['down_to_flat_error']
            error_counts['down_to_flat'] += 1
        elif true_label == 2 and pred_label == 0:
            # é¢„æµ‹å¹³å®é™…æ¶¨
            total_score += error_weights['flat_to_up_error']
            error_counts['flat_to_up'] += 1
        elif true_label == 2 and pred_label == 1:
            # é¢„æµ‹å¹³å®é™…è·Œ
            total_score += error_weights['flat_to_down_error']
            error_counts['flat_to_down'] += 1
    
    normalized_score = total_score / len(y_true)
    
    return {
        'trading_value_score': normalized_score,
        'total_score': total_score,
        'error_counts': error_counts
    }


def calculate_class_specific_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """
    è®¡ç®—å„ç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        
    Returns:
        å„ç±»åˆ«æŒ‡æ ‡
    """
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average=None, labels=[0, 1, 2], zero_division=0
    )
    
    return {
        'up_precision': precision[0],
        'down_precision': precision[1], 
        'flat_precision': precision[2],
        'up_recall': recall[0],
        'down_recall': recall[1],
        'flat_recall': recall[2],
        'up_f1': f1[0],
        'down_f1': f1[1],
        'flat_f1': f1[2],
        'up_support': support[0],
        'down_support': support[1],
        'flat_support': support[2]
    }


def comprehensive_evaluation(y_true: np.ndarray, y_pred: np.ndarray, 
                           error_weights: Dict[str, float]) -> Dict[str, any]:
    """
    ç»¼åˆè¯„ä¼°å‡½æ•°
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        error_weights: é”™è¯¯æƒé‡é…ç½®
        
    Returns:
        å®Œæ•´è¯„ä¼°ç»“æœ
    """
    results = {}
    
    # åŸºç¡€æŒ‡æ ‡
    results['overall_accuracy'] = accuracy_score(y_true, y_pred)
    
    # æ–¹å‘æ€§æŒ‡æ ‡
    results['directional_accuracy'] = calculate_directional_accuracy(y_true, y_pred)
    
    # ç¾éš¾æ€§é”™è¯¯
    catastrophic_metrics = calculate_catastrophic_error_rate(y_true, y_pred)
    results.update(catastrophic_metrics)
    
    # äº¤æ˜“ä»·å€¼è¯„åˆ†
    trading_metrics = calculate_trading_value_score(y_true, y_pred, error_weights)
    results.update(trading_metrics)
    
    # ç±»åˆ«ç‰¹å®šæŒ‡æ ‡
    class_metrics = calculate_class_specific_metrics(y_true, y_pred)
    results.update(class_metrics)
    
    # æ··æ·†çŸ©é˜µ
    results['confusion_matrix'] = confusion_matrix(y_true, y_pred)
    
    return results


def should_continue_training(metrics: Dict[str, float], config) -> Tuple[bool, str]:
    """
    æ™ºèƒ½æ—©åœåˆ¤æ–­
    
    Args:
        metrics: å½“å‰è¯„ä¼°æŒ‡æ ‡
        config: é…ç½®å¯¹è±¡
        
    Returns:
        (æ˜¯å¦ç»§ç»­è®­ç»ƒ, åœæ­¢åŸå› )
    """
    # æ£€æŸ¥ç¾éš¾æ€§é”™è¯¯ç‡æ˜¯å¦è¿‡é«˜
    if metrics['catastrophic_error_rate'] > config.max_catastrophic_error_rate:
        return True, f"ç¾éš¾é”™è¯¯ç‡è¿‡é«˜: {metrics['catastrophic_error_rate']:.3f}"
    
    # æ£€æŸ¥æ–¹å‘å‡†ç¡®ç‡æ˜¯å¦å¤ªä½
    if metrics['directional_accuracy'] < config.min_directional_accuracy:
        return True, f"æ–¹å‘å‡†ç¡®ç‡è¿‡ä½: {metrics['directional_accuracy']:.3f}"
    
    # æ£€æŸ¥æ¶¨è·Œç±»å¬å›ç‡
    if metrics.get('up_recall', 0) < 0.1 or metrics.get('down_recall', 0) < 0.1:
        return True, "æ¶¨è·Œç±»å¬å›ç‡è¿‡ä½ï¼Œç»§ç»­è®­ç»ƒ"
    
    return False, "è¯„ä¼°æŒ‡æ ‡æ»¡è¶³æ—©åœæ¡ä»¶"


def print_trading_evaluation_summary(results: Dict[str, any], epoch: int = None):
    """
    æ‰“å°äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°æ‘˜è¦
    
    Args:
        results: è¯„ä¼°ç»“æœ
        epoch: å½“å‰è½®æ¬¡
    """
    if epoch is not None:
        print(f"\nğŸ“Š Epoch {epoch} - äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°ç»“æœ")
    else:
        print("\nğŸ“Š äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°ç»“æœ")
    
    print("="*50)
    
    # æ ¸å¿ƒæŒ‡æ ‡
    print("ğŸ¯ æ ¸å¿ƒäº¤æ˜“æŒ‡æ ‡:")
    print(f"  æ–¹å‘å‡†ç¡®ç‡: {results['directional_accuracy']:.3f}")
    print(f"  ç¾éš¾é”™è¯¯ç‡: {results['catastrophic_error_rate']:.3f}")
    print(f"  äº¤æ˜“ä»·å€¼è¯„åˆ†: {results['trading_value_score']:.3f}")
    
    # é”™è¯¯åˆ†è§£
    print(f"\nğŸš¨ é”™è¯¯åˆ†æ:")
    print(f"  é¢„æµ‹æ¶¨â†’å®é™…è·Œ: {results['up_to_down_errors']}æ¬¡ ğŸ’¥")
    print(f"  é¢„æµ‹è·Œâ†’å®é™…æ¶¨: {results['down_to_up_errors']}æ¬¡ ğŸ’¥")
    
    error_counts = results['error_counts']
    print(f"  é¢„æµ‹æ¶¨â†’å®é™…å¹³: {error_counts['up_to_flat']}æ¬¡")
    print(f"  é¢„æµ‹è·Œâ†’å®é™…å¹³: {error_counts['down_to_flat']}æ¬¡")
    
    # ç±»åˆ«è¡¨ç°
    print(f"\nğŸ“ˆ ç±»åˆ«è¡¨ç°:")
    print(f"  ä¸Šæ¶¨ç±» - ç²¾ç¡®ç‡: {results['up_precision']:.3f}, å¬å›ç‡: {results['up_recall']:.3f}")
    print(f"  ä¸‹è·Œç±» - ç²¾ç¡®ç‡: {results['down_precision']:.3f}, å¬å›ç‡: {results['down_recall']:.3f}")
    print(f"  æŒå¹³ç±» - ç²¾ç¡®ç‡: {results['flat_precision']:.3f}, å¬å›ç‡: {results['flat_recall']:.3f}")
    
    # æ ·æœ¬åˆ†å¸ƒ
    print(f"\nğŸ“Š æ ·æœ¬åˆ†å¸ƒ:")
    print(f"  ä¸Šæ¶¨æ ·æœ¬: {results['up_support']}ä¸ª")
    print(f"  ä¸‹è·Œæ ·æœ¬: {results['down_support']}ä¸ª") 
    print(f"  æŒå¹³æ ·æœ¬: {results['flat_support']}ä¸ª")
    print(f"  æ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']:.3f}")
    
    print("="*50) 