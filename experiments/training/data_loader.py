"""
ğŸ§ª æ•°æ®åŠ è½½å™¨
ç‹¬ç«‹å®éªŒç¨‹åºï¼Œå¤„ç†aggTradesæ•°æ®ï¼Œåˆ›å»ºæ—¶åºåºåˆ—

ğŸ“Š æ•°æ®åŸºå‡†å¤‡å¿˜å½•ï¼š
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ å½“å‰é…ç½®ï¼š
- æ•°æ®æºï¼šaggTradesèšåˆæ•°æ®ï¼ˆbidä»·æ ¼åŸºå‡†ï¼‰
- ç‰¹å¾å·¥ç¨‹ï¼šAggTradesFeatureEngineer
- åºåˆ—é•¿åº¦ï¼š30åˆ†é’Ÿå†å²çª—å£
- é¢„æµ‹ç›®æ ‡ï¼š5/10/15åˆ†é’Ÿåçš„bidä»·æ ¼å˜åŒ–æ–¹å‘

ğŸ’° ä»·æ ¼åŸºå‡†ï¼š
- ä½¿ç”¨best_bidä½œä¸ºcloseä»·æ ¼
- é¢„æµ‹æœªæ¥æ—¶åˆ»çš„ä»·æ ¼å˜åŒ–ï¼ˆéæ—¶é—´æ®µï¼‰
- åˆ†ç±»é˜ˆå€¼ï¼šÂ±0.01%

ğŸ”§ æ•°æ®æµç¨‹ï¼š
1. åŠ è½½aggTrades CSVæ•°æ®
2. ä½¿ç”¨AggTradesFeatureEngineerå¤„ç†ç‰¹å¾
3. åˆ›å»º30åˆ†é’Ÿæ»‘åŠ¨çª—å£åºåˆ—
4. æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆä¿æŒæ—¶åºä¸€è‡´æ€§ï¼‰
5. ç”ŸæˆPyTorch DataLoader

âš–ï¸ ç±»åˆ«å¹³è¡¡å¤„ç†ï¼š
- æä¸å¹³è¡¡æ•°æ®ï¼ˆæŒå¹³95%+ï¼‰
- éœ€è¦é€‚å½“çš„é‡‡æ ·ç­–ç•¥
- è€ƒè™‘åŠ æƒæŸå¤±å‡½æ•°

æœ€åæ›´æ–°ï¼š2025-06-02
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import pickle
import warnings
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
from sklearn.preprocessing import RobustScaler, StandardScaler
from torch.utils.data import DataLoader, Dataset

warnings.filterwarnings('ignore')

class KLineDataset(Dataset):
    """Kçº¿æ—¶åºæ•°æ®é›†"""
    
    def __init__(self, features: np.ndarray, labels: Dict[str, np.ndarray], 
                 sequence_length: int, transform: Optional[callable] = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            features: ç‰¹å¾æ•°ç»„ [n_samples, n_features]
            labels: æ ‡ç­¾å­—å…¸ {horizon: [n_samples]}
            sequence_length: åºåˆ—é•¿åº¦
            transform: æ•°æ®å˜æ¢å‡½æ•°
        """
        self.features = features
        self.labels = labels
        self.sequence_length = sequence_length
        self.transform = transform
        
        # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°é‡ï¼ˆéœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
        total_samples = len(features) - sequence_length + 1
        
        # ğŸ”§ è¿‡æ»¤æ‰æ— æ•ˆæ ‡ç­¾ï¼ˆ-1ï¼‰çš„æ ·æœ¬
        self.valid_indices = []
        for idx in range(total_samples):
            label_idx = idx + sequence_length - 1
            # æ£€æŸ¥æ‰€æœ‰æ—¶é—´å°ºåº¦çš„æ ‡ç­¾æ˜¯å¦éƒ½æœ‰æ•ˆ
            all_valid = True
            for horizon, label_array in labels.items():
                if label_idx < len(label_array) and label_array[label_idx] == -1:
                    all_valid = False
                    break
            if all_valid:
                self.valid_indices.append(idx)
        
        self.n_samples = len(self.valid_indices)
        self.feature_dim = features.shape[1]
        
        print(f"  ğŸ“Š æ•°æ®é›†ç»Ÿè®¡: æ€»æ ·æœ¬={total_samples}, æœ‰æ•ˆæ ·æœ¬={self.n_samples} ({self.n_samples/total_samples*100:.1f}%)")
        
        # æ ‡ç­¾å¯¹é½æ£€æŸ¥
        for horizon, label_array in labels.items():
            if len(label_array) != len(features):
                raise ValueError(f"ç‰¹å¾å’Œæ ‡ç­¾{horizon}é•¿åº¦ä¸åŒ¹é…: {len(features)} vs {len(label_array)}")
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            (features, labels) å…ƒç»„
        """
        if idx >= self.n_samples:
            raise IndexError(f"ç´¢å¼•{idx}è¶…å‡ºèŒƒå›´{self.n_samples}")
        
        # è·å–æœ‰æ•ˆæ ·æœ¬çš„çœŸå®ç´¢å¼•
        real_idx = self.valid_indices[idx]
        
        # è·å–åºåˆ—ç‰¹å¾ [sequence_length, feature_dim]
        start_idx = real_idx
        end_idx = real_idx + self.sequence_length
        sequence_features = self.features[start_idx:end_idx]
        
        # è·å–å¯¹åº”çš„æ ‡ç­¾ï¼ˆä½¿ç”¨åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´ç‚¹çš„æ ‡ç­¾ï¼‰
        label_idx = end_idx - 1
        sample_labels = {}
        for horizon, label_array in self.labels.items():
            if label_idx < len(label_array):
                label_value = label_array[label_idx]
                # ç¡®ä¿ä¸æ˜¯æ— æ•ˆæ ‡ç­¾
                if label_value == -1:
                    raise ValueError(f"é‡åˆ°æ— æ•ˆæ ‡ç­¾ï¼Œç´¢å¼•={idx}, æ ‡ç­¾ç´¢å¼•={label_idx}")
                sample_labels[horizon] = label_value
            else:
                # å¦‚æœæ ‡ç­¾ä¸è¶³ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»è¿‡æ»¤äº†ï¼‰
                raise ValueError(f"æ ‡ç­¾ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼š{label_idx} >= {len(label_array)}")
        
        # è½¬æ¢ä¸ºtensor
        sequence_tensor = torch.FloatTensor(sequence_features)
        label_tensors = {k: torch.LongTensor([v]) for k, v in sample_labels.items()}
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            sequence_tensor = self.transform(sequence_tensor)
        
        return sequence_tensor, label_tensors

class KLineDataProcessor:
    """Kçº¿æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.feature_scaler = None
        self.feature_names = []
        self.label_encodings = {}
        
    def load_csv_data(self, csv_path: str) -> pd.DataFrame:
        """
        åŠ è½½CSVæ•°æ®
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŸå§‹æ•°æ®DataFrame
        """
        print(f"ğŸ“‚ åŠ è½½CSVæ•°æ®: {csv_path}")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        
        try:
            # å°è¯•è‡ªåŠ¨æ£€æµ‹CSVæ ¼å¼
            df = pd.read_csv(csv_path)
            print(f"  âœ… æˆåŠŸåŠ è½½ {len(df)} è¡Œæ•°æ®")
            print(f"  ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
            
            # æ£€æŸ¥aggTradeså¿…éœ€åˆ—
            required_cols = ['timestamp', 'best_bid', 'best_ask', 'total_volume']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"  âš ï¸ ç¼ºå°‘åˆ—: {missing_cols}")
                # å°è¯•æ˜ å°„åˆ—å
                df = self._map_column_names(df)
            
            return df
            
        except Exception as e:
            raise ValueError(f"CSVæ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    
    def _map_column_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ˜ å°„åˆ—ååˆ°æ ‡å‡†æ ¼å¼"""
        column_mapping = self.config.get('csv_columns', {})
        
        if column_mapping:
            df = df.rename(columns=column_mapping)
            print(f"  ğŸ”„ åˆ—åæ˜ å°„å®Œæˆ: {column_mapping}")
        
        return df
    
    def prepare_features_and_labels(self, df: pd.DataFrame) -> Tuple[np.ndarray, Dict[str, np.ndarray], List[str]]:
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        
        Args:
            df: åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„DataFrame
            
        Returns:
            (features, labels, feature_names)
        """
        # ä½¿ç”¨æ–°çš„AggTradesFeatureEngineer
        import os
        import sys
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from utils.aggtrades_features import AggTradesFeatureEngineer
        
        print("ğŸ”§ å‡†å¤‡aggTradesç‰¹å¾å’Œæ ‡ç­¾...")
        
        # ä½¿ç”¨aggTradesç‰¹å¾å·¥ç¨‹
        fe = AggTradesFeatureEngineer()
        features_df = fe.process_aggtrades_data(df)
        
        # åˆ›å»ºæ ‡ç­¾
        horizons = self.config.get('prediction_horizons', [5, 10, 15])
        threshold = self.config.get('price_change_threshold', 0.0001)
        use_two_task = self.config.get('use_two_task_mode', False)
        labels_df = fe.create_labels(features_df, horizons, threshold, two_task_mode=use_two_task)
        
        # æå–ç‰¹å¾çŸ©é˜µ
        feature_names = fe.get_feature_names()
        features = labels_df[feature_names].values
        
        # æå–æ ‡ç­¾ï¼ˆæ”¯æŒä¸¤ä»»åŠ¡æ¨¡å¼ï¼‰
        labels = {}
        if use_two_task:
            # ä¸¤ä»»åŠ¡æ¨¡å¼ï¼šåˆ†åˆ«æå–ç¨³å®šæ€§å’Œæ–¹å‘æ ‡ç­¾
            for horizon in horizons:
                stability_col = f'stability_{horizon}min'
                direction_col = f'direction_{horizon}min'
                if stability_col in labels_df.columns:
                    labels[f'stability_{horizon}min'] = labels_df[stability_col].values
                if direction_col in labels_df.columns:
                    labels[f'direction_{horizon}min'] = labels_df[direction_col].values
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šä¸‰åˆ†ç±»æ ‡ç­¾
            for horizon in horizons:
                label_col = f'label_{horizon}min'
                if label_col in labels_df.columns:
                    labels[f'{horizon}min'] = labels_df[label_col].values
        
        print(f"  âœ… ç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"  âœ… æ ‡ç­¾æ•°é‡: {len(labels)}")
        for horizon, label_array in labels.items():
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨pandasç»Ÿè®¡ï¼Œå¯ä»¥å¤„ç†-1æ ‡ç­¾
            label_counts = pd.Series(label_array).value_counts().sort_index()
            print(f"    {horizon}: {label_array.shape}, ç±»åˆ«åˆ†å¸ƒ: {label_counts.to_dict()}")
        
        return features, labels, feature_names
    
    def split_data(self, features: np.ndarray, labels: Dict[str, np.ndarray]) -> Tuple[
        Tuple[np.ndarray, Dict[str, np.ndarray]],  # train
        Tuple[np.ndarray, Dict[str, np.ndarray]],  # val  
        Tuple[np.ndarray, Dict[str, np.ndarray]]   # test
    ]:
        """
        æ—¶åºæ•°æ®åˆ†å‰²ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
        
        Args:
            features: ç‰¹å¾æ•°ç»„
            labels: æ ‡ç­¾å­—å…¸
            
        Returns:
            (train, val, test) æ•°æ®å…ƒç»„
        """
        print("ğŸ“Š åˆ†å‰²è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†...")
        
        n_samples = len(features)
        train_ratio = self.config.get('train_ratio', 0.8)
        val_ratio = self.config.get('val_ratio', 0.1)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))
        
        # åˆ†å‰²ç‰¹å¾
        train_features = features[:train_end]
        val_features = features[train_end:val_end]
        test_features = features[val_end:]
        
        # åˆ†å‰²æ ‡ç­¾
        train_labels = {k: v[:train_end] for k, v in labels.items()}
        val_labels = {k: v[train_end:val_end] for k, v in labels.items()}
        test_labels = {k: v[val_end:] for k, v in labels.items()}
        
        print(f"  ğŸ“ˆ è®­ç»ƒé›†: {len(train_features)} æ ·æœ¬")
        print(f"  ğŸ“Š éªŒè¯é›†: {len(val_features)} æ ·æœ¬")
        print(f"  ğŸ“‹ æµ‹è¯•é›†: {len(test_features)} æ ·æœ¬")
        
        return (train_features, train_labels), (val_features, val_labels), (test_features, test_labels)
    
    def fit_scaler(self, train_features: np.ndarray) -> None:
        """
        æ‹Ÿåˆç‰¹å¾ç¼©æ”¾å™¨
        
        Args:
            train_features: è®­ç»ƒé›†ç‰¹å¾
        """
        print("ğŸ“ æ‹Ÿåˆç‰¹å¾ç¼©æ”¾å™¨...")
        
        # ä½¿ç”¨RobustScalerï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
        self.feature_scaler = RobustScaler()
        
        # åªåœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆ
        self.feature_scaler.fit(train_features)
        
        print("  âœ… ç¼©æ”¾å™¨æ‹Ÿåˆå®Œæˆ")
    
    def transform_features(self, features: np.ndarray) -> np.ndarray:
        """
        åº”ç”¨ç‰¹å¾ç¼©æ”¾
        
        Args:
            features: åŸå§‹ç‰¹å¾
            
        Returns:
            ç¼©æ”¾åçš„ç‰¹å¾
        """
        if self.feature_scaler is None:
            raise ValueError("ç¼©æ”¾å™¨æœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit_scaler()")
        
        return self.feature_scaler.transform(features)
    
    def create_data_loaders(self, train_data: Tuple, val_data: Tuple, test_data: Tuple) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        åˆ›å»ºæ•°æ®åŠ è½½å™¨
        
        Args:
            train_data: è®­ç»ƒæ•°æ® (features, labels)
            val_data: éªŒè¯æ•°æ® (features, labels)
            test_data: æµ‹è¯•æ•°æ® (features, labels)
            
        Returns:
            (train_loader, val_loader, test_loader)
        """
        print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        sequence_length = self.config.get('sequence_length', 30)
        batch_size = self.config.get('batch_size', 64)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = KLineDataset(train_data[0], train_data[1], sequence_length)
        val_dataset = KLineDataset(val_data[0], val_data[1], sequence_length)
        test_dataset = KLineDataset(test_data[0], test_data[1], sequence_length)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,  # è®­ç»ƒæ—¶éšæœºæ‰“ä¹±
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,  # éªŒè¯æ—¶ä¿æŒé¡ºåº
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size, 
            shuffle=False,  # æµ‹è¯•æ—¶ä¿æŒé¡ºåº
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        print(f"  âœ… è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  âœ… éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        print(f"  âœ… æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
        return train_loader, val_loader, test_loader
    
    def save_preprocessor(self, save_path: str) -> None:
        """ä¿å­˜é¢„å¤„ç†å™¨çŠ¶æ€"""
        preprocessor_state = {
            'feature_scaler': self.feature_scaler,
            'feature_names': self.feature_names,
            'config': self.config
        }
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump(preprocessor_state, f)
        
        print(f"ğŸ’¾ é¢„å¤„ç†å™¨çŠ¶æ€å·²ä¿å­˜: {save_path}")
    
    def load_preprocessor(self, load_path: str) -> None:
        """åŠ è½½é¢„å¤„ç†å™¨çŠ¶æ€"""
        with open(load_path, 'rb') as f:
            preprocessor_state = pickle.load(f)
        
        self.feature_scaler = preprocessor_state['feature_scaler']
        self.feature_names = preprocessor_state['feature_names']
        
        print(f"ğŸ“‚ é¢„å¤„ç†å™¨çŠ¶æ€å·²åŠ è½½: {load_path}")

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    print("ğŸ§ª åˆ›å»ºç¤ºä¾‹Kçº¿æ•°æ®...")
    
    # ç”Ÿæˆ1å¹´çš„åˆ†é’Ÿçº§æ•°æ®
    np.random.seed(42)
    n_samples = 365 * 24 * 60  # 1å¹´çš„åˆ†é’Ÿæ•°
    
    dates = pd.date_range('2024-01-01', periods=n_samples, freq='1min')
    
    # ç”Ÿæˆé€¼çœŸçš„OHLCVæ•°æ®
    base_price = 1.0856
    prices = []
    volumes = []
    
    current_price = base_price
    for i in range(n_samples):
        # ä»·æ ¼éšæœºæ¸¸èµ°
        price_change = np.random.randn() * 0.0001
        current_price += price_change
        
        # ç¡®ä¿ä»·æ ¼åœ¨åˆç†èŒƒå›´å†…
        current_price = max(0.5, min(2.0, current_price))
        
        # ç”ŸæˆOHLC
        high_offset = abs(np.random.randn()) * 0.0002
        low_offset = abs(np.random.randn()) * 0.0002
        close_change = np.random.randn() * 0.0001
        
        open_price = current_price
        high_price = current_price + high_offset
        low_price = current_price - low_offset
        close_price = current_price + close_change
        
        # ç¡®ä¿OHLCå…³ç³»æ­£ç¡®
        high_price = max(high_price, open_price, close_price)
        low_price = min(low_price, open_price, close_price)
        
        prices.append([open_price, high_price, low_price, close_price])
        
        # ç”Ÿæˆæˆäº¤é‡
        volume = np.random.randint(10000, 100000)
        volumes.append(volume)
        
        current_price = close_price
    
    # åˆ›å»ºDataFrame
    prices = np.array(prices)
    sample_data = pd.DataFrame({
        'timestamp': dates,
        'open': prices[:, 0],
        'high': prices[:, 1], 
        'low': prices[:, 2],
        'close': prices[:, 3],
        'volume': volumes
    })
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs('experiments/data/raw', exist_ok=True)
    sample_path = 'experiments/data/raw/sample_kline_1year.csv'
    sample_data.to_csv(sample_path, index=False)
    
    print(f"âœ… ç¤ºä¾‹æ•°æ®å·²åˆ›å»º: {sample_path}")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {sample_data.shape}")
    print(f"ğŸ“ˆ ä»·æ ¼èŒƒå›´: {sample_data['close'].min():.4f} - {sample_data['close'].max():.4f}")
    
    return sample_path

def test_data_loader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sample_path = create_sample_data()
    
    # é…ç½®
    config = {
        'data_file': sample_path,
        'sequence_length': 30,
        'prediction_horizons': [5, 10, 15],
        'price_change_threshold': 0.001,
        'train_ratio': 0.8,
        'val_ratio': 0.1,
        'test_ratio': 0.1,
        'batch_size': 64
    }
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = KLineDataProcessor(config)
    
    # åŠ è½½å’Œå¤„ç†æ•°æ®
    df = processor.load_csv_data(sample_path)
    features, labels, feature_names = processor.prepare_features_and_labels(df)
    
    # åˆ†å‰²æ•°æ®
    train_data, val_data, test_data = processor.split_data(features, labels)
    
    # æ‹Ÿåˆç¼©æ”¾å™¨
    processor.fit_scaler(train_data[0])
    
    # åº”ç”¨ç¼©æ”¾
    train_features = processor.transform_features(train_data[0])
    val_features = processor.transform_features(val_data[0])
    test_features = processor.transform_features(test_data[0])
    
    train_data = (train_features, train_data[1])
    val_data = (val_features, val_data[1])
    test_data = (test_features, test_data[1])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = processor.create_data_loaders(
        train_data, val_data, test_data
    )
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    for batch_features, batch_labels in train_loader:
        print(f"æ‰¹æ¬¡ç‰¹å¾å½¢çŠ¶: {batch_features.shape}")
        print(f"æ‰¹æ¬¡æ ‡ç­¾:")
        for horizon, labels_tensor in batch_labels.items():
            print(f"  {horizon}: {labels_tensor.shape}")
        break
    
    print("âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ")
    return train_loader, val_loader, test_loader

if __name__ == "__main__":
    test_data_loader() 