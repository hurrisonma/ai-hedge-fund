#!/usr/bin/env python3
"""
ğŸ§ª æ·±åº¦å­¦ä¹ å®éªŒä¸»ç¨‹åº
ç‹¬ç«‹å®éªŒç¨‹åºï¼Œä¸å½±å“ç°æœ‰å·¥ç¨‹

å®Œæ•´çš„Kçº¿æ•°æ®æ·±åº¦å­¦ä¹ é¢„æµ‹å®éªŒ
ä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒå†åˆ°ç»“æœè¯„ä¼°çš„ç«¯åˆ°ç«¯æµç¨‹
"""

import argparse
import os
import sys
import time
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.transformer import (  # noqa: E402
    BinaryClassificationLoss,
    MarketTransformer,
    MultiTaskLoss,
    TradingAwareLoss,
)
from training.config import ExperimentConfig  # noqa: E402
from training.data_loader import create_sample_data, KLineDataProcessor  # noqa: E402

warnings.filterwarnings('ignore')


class DeepLearningExperiment:
    """æ·±åº¦å­¦ä¹ å®éªŒä¸»æ§åˆ¶å™¨"""
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.device = self._get_device()
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_processor = None
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.writer = None

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_accuracy = 0.0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_accuracy': [],
            'val_accuracy': []
        }

        print("ğŸ§ª å®éªŒåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(config.summary())

    def _get_device(self) -> torch.device:
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  ä½¿ç”¨CPU")
        return device

    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("\n" + "="*50)
        print("ğŸ“Š æ•°æ®å‡†å¤‡é˜¶æ®µ")
        print("="*50)

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.config.data_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.config.data_file}")
            print("ğŸ§ª åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œå®éªŒ...")
            sample_path = create_sample_data()
            self.config.data_file = sample_path

        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        config_dict = {
            'data_file': self.config.data_file,
            'sequence_length': self.config.sequence_length,
            'prediction_horizons': self.config.prediction_horizons,
            'price_change_threshold': self.config.price_change_threshold,
            'train_ratio': self.config.train_ratio,
            'val_ratio': self.config.val_ratio,
            'test_ratio': self.config.test_ratio,
            'batch_size': self.config.batch_size
        }

        self.data_processor = KLineDataProcessor(config_dict)

        # åŠ è½½åŸå§‹æ•°æ®
        df = self.data_processor.load_csv_data(self.config.data_file)

        # ç‰¹å¾å·¥ç¨‹å’Œæ ‡ç­¾åˆ›å»º
        features, labels, feature_names = (
            self.data_processor.prepare_features_and_labels(df)
        )

        # æ•°æ®åˆ†å‰²
        train_data, val_data, test_data = (
            self.data_processor.split_data(features, labels)
        )

        # ç‰¹å¾ç¼©æ”¾
        self.data_processor.fit_scaler(train_data[0])
        train_features = self.data_processor.transform_features(train_data[0])
        val_features = self.data_processor.transform_features(val_data[0])
        test_features = self.data_processor.transform_features(test_data[0])

        train_data = (train_features, train_data[1])
        val_data = (val_features, val_data[1])
        test_data = (test_features, test_data[1])

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        (self.train_loader, self.val_loader,
         self.test_loader) = self.data_processor.create_data_loaders(
            train_data, val_data, test_data
        )

        # ä¿å­˜é¢„å¤„ç†å™¨
        preprocessor_path = os.path.join(
            self.config.output_dir, 'preprocessor.pkl'
        )
        self.data_processor.save_preprocessor(preprocessor_path)

        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")

    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("\n" + "="*50)
        print("ğŸ—ï¸  æ¨¡å‹æ„å»ºé˜¶æ®µ")
        print("="*50)

        # æ¨¡å‹é…ç½®
        model_config = {
            'feature_dim': self.config.feature_dim,
            'embed_dim': self.config.transformer_config['embed_dim'],
            'num_heads': self.config.transformer_config['num_heads'],
            'num_layers': self.config.transformer_config['num_layers'],
            'ff_dim': self.config.transformer_config['ff_dim'],
            'dropout': self.config.transformer_config['dropout'],
            'sequence_length': self.config.sequence_length,
            'prediction_horizons': self.config.prediction_horizons
        }

        # åˆ›å»ºæ¨¡å‹
        self.model = MarketTransformer(model_config).to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.lr_scheduler == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.lr_scheduler_params['T_max'],
                eta_min=self.config.lr_scheduler_params['eta_min']
            )

        # æŸå¤±å‡½æ•°
        if self.config.use_binary_classification:
            # äºŒåˆ†ç±»æ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„ç±»åˆ«æƒé‡
            binary_class_weights = torch.FloatTensor(
                self.config.class_weights
            ).to(self.device)
            self.criterion = BinaryClassificationLoss(
                self.config.task_weights, binary_class_weights
            )
        else:
            # ä¸‰åˆ†ç±»æ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
            class_weights = torch.FloatTensor(
                self.config.class_weights
            ).to(self.device)

            if self.config.use_trading_aware_loss:
                # ä½¿ç”¨äº¤æ˜“æ„ŸçŸ¥æŸå¤±å‡½æ•°
                self.criterion = TradingAwareLoss(
                    self.config.task_weights,
                    self.config.direction_error_matrix,
                    class_weights
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿå¤šä»»åŠ¡æŸå¤±å‡½æ•°
                self.criterion = MultiTaskLoss(
                    self.config.task_weights, class_weights
                )

        # TensorBoard
        self.writer = SummaryWriter(self.config.log_dir)

        # æ¨¡å‹ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(
            p.numel() for p in self.model.parameters() if p.requires_grad
        )

        print("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.1f}MB")
        print("âœ… æ¨¡å‹æ„å»ºå®Œæˆ")

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        correct_predictions = {
            f'{h}min': 0 for h in self.config.prediction_horizons
        }

        for batch_idx, (features, labels) in enumerate(self.train_loader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            features = features.to(self.device)
            batch_labels = {}
            for horizon, label_tensor in labels.items():
                batch_labels[horizon] = label_tensor.squeeze().to(
                    self.device
                )

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions = self.model(features)

            # è®¡ç®—æŸå¤±
            losses = self.criterion(predictions, batch_labels)
            total_loss += losses['total_loss'].item()

            # åå‘ä¼ æ’­
            losses['total_loss'].backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), max_norm=1.0
            )

            self.optimizer.step()

            # è®¡ç®—å‡†ç¡®ç‡
            batch_size = features.size(0)
            total_samples += batch_size

            for horizon in self.config.prediction_horizons:
                horizon_key = f'{horizon}min'
                pred_labels = torch.argmax(predictions[horizon_key], dim=1)
                correct = (
                    pred_labels == batch_labels[horizon_key]
                ).sum().item()
                correct_predictions[horizon_key] += correct

            # è®°å½•æ—¥å¿—
            if batch_idx % self.config.log_every_n_steps == 0:
                avg_loss = total_loss / (batch_idx + 1)
                print(
                    f"  Batch {batch_idx:4d}/{len(self.train_loader)}: "
                    f"Loss={avg_loss:.4f}"
                )

        # è®¡ç®—epochç»Ÿè®¡
        avg_loss = total_loss / len(self.train_loader)
        accuracies = {
            k: v / total_samples for k, v in correct_predictions.items()
        }
        avg_accuracy = np.mean(list(accuracies.values()))

        return avg_loss, avg_accuracy, accuracies

    def validate_epoch(self):
        """éªŒè¯ä¸€ä¸ªepoch - ä½¿ç”¨ç®€å•äºŒåˆ†ç±»è¯„ä¼°"""
        self.model.eval()
        total_loss = 0.0
        all_predictions = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }
        all_labels = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }

        with torch.no_grad():
            for batch_idx, (features, labels) in enumerate(self.val_loader):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                features = features.to(self.device)
                batch_labels = {}
                for horizon, label_tensor in labels.items():
                    batch_labels[horizon] = label_tensor.squeeze().to(
                        self.device
                    )

                # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
                if features.size(0) == 0:
                    print(f"âš ï¸  è­¦å‘Š: æ‰¹æ¬¡ {batch_idx} ç‰¹å¾ä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä¸ºç©ºæˆ–å°ºå¯¸ä¸åŒ¹é…
                skip_batch = False
                for horizon_key, label_tensor in batch_labels.items():
                    # ä½¿ç”¨numel()æ£€æŸ¥å¼ é‡æ˜¯å¦ä¸ºç©ºï¼Œå¤„ç†0ç»´å¼ é‡
                    if label_tensor.numel() == 0:
                        print(
                            f"âš ï¸  è­¦å‘Š: æ‰¹æ¬¡ {batch_idx} {horizon_key} "
                            f"æ ‡ç­¾ä¸ºç©ºï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡"
                        )
                        skip_batch = True
                        break
                    # æ£€æŸ¥ç»´åº¦åŒ¹é…
                    if (len(label_tensor.shape) == 0 or
                            label_tensor.shape[0] != features.shape[0]):
                        print(
                            f"âš ï¸  è­¦å‘Š: æ‰¹æ¬¡ {batch_idx} {horizon_key} "
                            f"å°ºå¯¸ä¸åŒ¹é…"
                        )
                        print(
                            f"    ç‰¹å¾å½¢çŠ¶: {features.shape}, "
                            f"æ ‡ç­¾å½¢çŠ¶: {label_tensor.shape}"
                        )
                        skip_batch = True
                        break

                if skip_batch:
                    continue

                # å‰å‘ä¼ æ’­
                predictions = self.model(features)

                # é¢å¤–çš„å®‰å…¨æ£€æŸ¥
                try:
                    losses = self.criterion(predictions, batch_labels)
                    total_loss += losses['total_loss'].item()
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡ {batch_idx} æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                    print(f"    ç‰¹å¾å½¢çŠ¶: {features.shape}")
                    for k, v in batch_labels.items():
                        print(f"    {k} æ ‡ç­¾å½¢çŠ¶: {v.shape}")
                    for k, v in predictions.items():
                        print(f"    {k} é¢„æµ‹å½¢çŠ¶: {v.shape}")
                    raise

                # æ”¶é›†é¢„æµ‹ç»“æœ
                for horizon in self.config.prediction_horizons:
                    horizon_key = f'{horizon}min'
                    pred_labels = torch.argmax(predictions[horizon_key], dim=1)
                    all_predictions[horizon_key].extend(
                        pred_labels.cpu().numpy()
                    )
                    all_labels[horizon_key].extend(
                        batch_labels[horizon_key].cpu().numpy()
                    )

        # è®¡ç®—ç®€å•äºŒåˆ†ç±»è¯„ä¼°æŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)

        # å¯¹æ¯ä¸ªæ—¶é—´å°ºåº¦è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import (
            accuracy_score,
            f1_score,
            precision_score,
            recall_score,
        )
        
        evaluation_results = {}
        for horizon in self.config.prediction_horizons:
            horizon_key = f'{horizon}min'
            y_true = np.array(all_labels[horizon_key])
            y_pred = np.array(all_predictions[horizon_key])

            # è®¡ç®—åŸºç¡€åˆ†ç±»æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
            
            evaluation_results[horizon_key] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_accuracy = np.mean([
            results['accuracy'] for results in evaluation_results.values()
        ])
        avg_precision = np.mean([
            results['precision'] for results in evaluation_results.values()
        ])
        avg_f1 = np.mean([
            results['f1_score'] for results in evaluation_results.values()
        ])

        # è¿”å›ä¸»è¦æŒ‡æ ‡
        metrics = {
            'loss': avg_loss,
            'accuracy': avg_accuracy,
            'precision': avg_precision,
            'f1_score': avg_f1,
            'detailed_results': evaluation_results
        }

        return metrics

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*50)
        print("ğŸš€ æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
        print("="*50)

        # æ—©åœç›¸å…³å˜é‡
        best_metrics = {
            'accuracy': 0.0,
            'precision': 0.0,
            'f1_score': 0.0
        }
        patience_counter = 0

        for epoch in range(self.config.max_epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()

            print(f"\nğŸ“ˆ Epoch {epoch+1}/{self.config.max_epochs}")
            print("-" * 40)

            # è®­ç»ƒ
            train_loss, train_accuracy, train_accuracies = (
                self.train_epoch(epoch)
            )

            # éªŒè¯
            val_metrics = self.validate_epoch()

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()

            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_metrics['loss'])
            self.training_history['train_accuracy'].append(train_accuracy)
            self.training_history['val_accuracy'].append(
                val_metrics['accuracy']
            )

            # TensorBoardè®°å½•
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            self.writer.add_scalar(
                'Loss/Validation', val_metrics['loss'], epoch
            )
            self.writer.add_scalar('Accuracy/Train', train_accuracy, epoch)
            self.writer.add_scalar(
                'Accuracy/Validation',
                val_metrics['accuracy'], epoch
            )

            # æ‰“å°è®­ç»ƒç»“æœ
            epoch_time = time.time() - epoch_start_time
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.3f}")
            print(f"éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")

            # æ‰“å°è¯„ä¼°ç»“æœ
            print("\nğŸ¯ ç¨³å®šæ€§æ£€æµ‹è¯„ä¼°:")
            print(f"  å‡†ç¡®ç‡: {val_metrics['accuracy']:.3f}")
            print(f"  ç²¾ç¡®ç‡: {val_metrics['precision']:.3f}")
            print(f"  F1åˆ†æ•°: {val_metrics['f1_score']:.3f}")

            print("å„æ—¶é—´å°ºåº¦è¡¨ç°:")
            for horizon, results in val_metrics['detailed_results'].items():
                print(
                    f"  {horizon}: "
                    f"å‡†ç¡®ç‡={results['accuracy']:.3f}, "
                    f"ç²¾ç¡®ç‡={results['precision']:.3f}, "
                    f"å¬å›ç‡={results['recall']:.3f}, "
                    f"F1åˆ†æ•°={results['f1_score']:.3f}"
                )

            print(f"è€—æ—¶: {epoch_time:.2f}s")

            # æ¨¡å‹ä¿å­˜é€»è¾‘ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰
            if val_metrics['accuracy'] > best_metrics['accuracy']:
                best_metrics['accuracy'] = val_metrics['accuracy']
                self.save_model('best_model.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³å‡†ç¡®ç‡æ¨¡å‹")
                patience_counter = 0
            else:
                patience_counter += 1

            # æ—©åœåˆ¤æ–­
            if patience_counter >= self.config.early_stopping_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘: è¿ç»­{patience_counter}è½®å‡†ç¡®ç‡æœªæå‡")
                break

            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.config.save_every_n_epochs == 0:
                self.save_model(f'checkpoint_epoch_{epoch+1}.pth')

        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³å‡†ç¡®ç‡: {best_metrics['accuracy']:.3f}")

    def evaluate(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
        print("\n" + "="*50)
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°é˜¶æ®µ")
        print("="*50)

        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(
            self.config.model_save_dir, 'best_model.pth'
        )
        if os.path.exists(best_model_path):
            self.model.load_state_dict(torch.load(
                best_model_path, map_location=self.device
            ))
            print("ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹")

        self.model.eval()
        all_predictions = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }
        all_labels = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }
        all_probabilities = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }

        with torch.no_grad():
            for features, labels in self.test_loader:
                features = features.to(self.device)

                # é¢„æµ‹
                predictions = self.model(features)
                probabilities = self.model.predict_proba(features)

                for horizon in self.config.prediction_horizons:
                    horizon_key = f'{horizon}min'

                    # é¢„æµ‹æ ‡ç­¾
                    pred_labels = torch.argmax(
                        predictions[horizon_key], dim=1
                    )
                    all_predictions[horizon_key].extend(
                        pred_labels.cpu().numpy()
                    )

                    # çœŸå®æ ‡ç­¾
                    true_labels = labels[horizon_key].squeeze()
                    all_labels[horizon_key].extend(true_labels.numpy())

                    # é¢„æµ‹æ¦‚ç‡
                    probs = probabilities[horizon_key]
                    all_probabilities[horizon_key].extend(
                        probs.cpu().numpy()
                    )

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        from sklearn.metrics import (
            accuracy_score,
            confusion_matrix,
            precision_recall_fscore_support,
        )

        results = {}
        for horizon in self.config.prediction_horizons:
            horizon_key = f'{horizon}min'

            y_true = np.array(all_labels[horizon_key])
            y_pred = np.array(all_predictions[horizon_key])
            y_proba = np.array(all_probabilities[horizon_key])

            # åŸºç¡€æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            (precision, recall,
             f1, _) = precision_recall_fscore_support(
                y_true, y_pred, average='weighted'
            )
            cm = confusion_matrix(y_true, y_pred)

            results[horizon_key] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'confusion_matrix': cm,
                'probabilities': y_proba
            }

            print(f"\n{horizon_key} é¢„æµ‹ç»“æœ:")
            print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
            print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
            print(f"  å¬å›ç‡: {recall:.3f}")
            print(f"  F1åˆ†æ•°: {f1:.3f}")
            print(f"  æ··æ·†çŸ©é˜µ:\n{cm}")

        return results

    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.config.model_save_dir, filename)
        torch.save(self.model.state_dict(), model_path)

    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.config.plot_training_curves:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(self.training_history['val_loss'], label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('æŸå¤±æ›²çº¿')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.training_history['train_accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
        axes[0, 1].plot(self.training_history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        plt.tight_layout()
        plot_path = os.path.join(self.config.plot_dir, 'training_curves.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")

    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        try:
            print("ğŸ§ª å¼€å§‹æ·±åº¦å­¦ä¹ å®éªŒ")
            print("=" * 60)

            # 1. æ•°æ®å‡†å¤‡
            self.prepare_data()

            # 2. æ¨¡å‹æ„å»º
            self.build_model()

            # 3. æ¨¡å‹è®­ç»ƒ
            self.train()

            # 4. æ¨¡å‹è¯„ä¼°
            results = self.evaluate()

            # 5. å¯è§†åŒ–
            self.plot_training_curves()

            print("\n" + "=" * 60)
            print("ğŸ‰ å®éªŒå®Œæˆï¼")
            print("=" * 60)

            return results

        except Exception as e:
            print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
            raise
        finally:
            if self.writer:
                self.writer.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦å­¦ä¹ å®éªŒç¨‹åº')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data', type=str, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    config = ExperimentConfig()

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.data:
        config.data_file = args.data
    if args.epochs:
        config.max_epochs = args.epochs
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.lr:
        config.learning_rate = args.lr

    # è¿è¡Œå®éªŒ
    experiment = DeepLearningExperiment(config)
    results = experiment.run_experiment()

    return results


if __name__ == "__main__":
    main()
