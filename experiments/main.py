#!/usr/bin/env python3
"""
ğŸ§ª æ·±åº¦å­¦ä¹ å®éªŒä¸»ç¨‹åº
ç‹¬ç«‹å®éªŒç¨‹åºï¼Œä¸å½±å“ç°æœ‰å·¥ç¨‹

å®Œæ•´çš„Kçº¿æ•°æ®æ·±åº¦å­¦ä¹ é¢„æµ‹å®éªŒ
ä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒå†åˆ°ç»“æœè¯„ä¼°çš„ç«¯åˆ°ç«¯æµç¨‹
"""

import argparse
import os
import sys
import time
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.transformer import (  # noqa: E402
    BinaryClassificationLoss,
    MarketTransformer,
    MultiTaskLoss,
    TradingAwareLoss,
)

from training.config import ExperimentConfig  # noqa: E402
from training.data_loader import KLineDataProcessor, create_sample_data  # noqa: E402

warnings.filterwarnings('ignore')


class DeepLearningExperiment:
    """æ·±åº¦å­¦ä¹ å®éªŒä¸»æ§åˆ¶å™¨"""
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.device = self._get_device()
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_processor = None
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.writer = None

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_accuracy = 0.0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_accuracy': [],
            'val_accuracy': []
        }

        print("ğŸ§ª å®éªŒåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(config.summary())

    def _get_device(self) -> torch.device:
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  ä½¿ç”¨CPU")
        return device

    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("\n" + "="*50)
        print("ğŸ“Š æ•°æ®å‡†å¤‡é˜¶æ®µ")
        print("="*50)

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.config.data_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.config.data_file}")
            print("ğŸ§ª åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œå®éªŒ...")
            sample_path = create_sample_data()
            self.config.data_file = sample_path

        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        config_dict = {
            'data_file': self.config.data_file,
            'sequence_length': self.config.sequence_length,
            'prediction_horizons': self.config.prediction_horizons,
            'price_change_threshold': self.config.price_change_threshold,
            'train_ratio': self.config.train_ratio,
            'val_ratio': self.config.val_ratio,
            'test_ratio': self.config.test_ratio,
            'batch_size': self.config.batch_size
        }

        self.data_processor = KLineDataProcessor(config_dict)

        # åŠ è½½åŸå§‹æ•°æ®
        df = self.data_processor.load_csv_data(self.config.data_file)

        # ç‰¹å¾å·¥ç¨‹å’Œæ ‡ç­¾åˆ›å»º
        features, labels, feature_names = (
            self.data_processor.prepare_features_and_labels(df)
        )

        # æ•°æ®åˆ†å‰²
        train_data, val_data, test_data = (
            self.data_processor.split_data(features, labels)
        )

        # ç‰¹å¾ç¼©æ”¾
        self.data_processor.fit_scaler(train_data[0])
        train_features = self.data_processor.transform_features(train_data[0])
        val_features = self.data_processor.transform_features(val_data[0])
        test_features = self.data_processor.transform_features(test_data[0])

        train_data = (train_features, train_data[1])
        val_data = (val_features, val_data[1])
        test_data = (test_features, test_data[1])

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        (self.train_loader, self.val_loader,
         self.test_loader) = self.data_processor.create_data_loaders(
            train_data, val_data, test_data
        )

        # ä¿å­˜é¢„å¤„ç†å™¨
        preprocessor_path = os.path.join(
            self.config.output_dir, 'preprocessor.pkl'
        )
        self.data_processor.save_preprocessor(preprocessor_path)

        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")

    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("\n" + "="*50)
        print("ğŸ—ï¸  æ¨¡å‹æ„å»ºé˜¶æ®µ")
        print("="*50)

        # æ¨¡å‹é…ç½®
        model_config = {
            'feature_dim': self.config.feature_dim,
            'embed_dim': self.config.transformer_config['embed_dim'],
            'num_heads': self.config.transformer_config['num_heads'],
            'num_layers': self.config.transformer_config['num_layers'],
            'ff_dim': self.config.transformer_config['ff_dim'],
            'dropout': self.config.transformer_config['dropout'],
            'sequence_length': self.config.sequence_length,
            'prediction_horizons': self.config.prediction_horizons
        }

        # åˆ›å»ºæ¨¡å‹
        self.model = MarketTransformer(model_config).to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.lr_scheduler == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.lr_scheduler_params['T_max'],
                eta_min=self.config.lr_scheduler_params['eta_min']
            )

        # æŸå¤±å‡½æ•°
        if self.config.use_binary_classification:
            # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°ç±»å‹
            loss_type = self.config.loss_function_type
            task_weights = self.config.task_weights
            
            if loss_type == "binary_cross_entropy":
                # æ ‡å‡†äºŒåˆ†ç±»äº¤å‰ç†µï¼ˆé»˜è®¤ï¼‰
                binary_class_weights = torch.FloatTensor(
                    self.config.class_weights
                ).to(self.device)
                self.criterion = BinaryClassificationLoss(
                    task_weights, binary_class_weights
                )
                
            elif loss_type == "probability_adjusted":
                # åŸºç¡€æ¦‚ç‡è°ƒæ•´æŸå¤±
                from models.transformer import ProbabilityAdjustedLoss
                params = self.config.loss_function_params["probability_adjusted"]
                self.criterion = ProbabilityAdjustedLoss(
                    task_weights,
                    base_stable_prob=params["base_stable_prob"],
                    base_change_prob=params["base_change_prob"]
                )
                
            elif loss_type == "confidence_weighted":
                # ç½®ä¿¡åº¦åŠ¨æ€æƒé‡æŸå¤±
                from models.transformer import ConfidenceWeightedLoss
                params = self.config.loss_function_params["confidence_weighted"]
                self.criterion = ConfidenceWeightedLoss(
                    task_weights,
                    confidence_threshold=params["confidence_threshold"],
                    high_conf_correct_weight=params["high_conf_correct_weight"],
                    high_conf_wrong_weight=params["high_conf_wrong_weight"],
                    low_conf_weight=params["low_conf_weight"]
                )
                
            elif loss_type == "business_cost":
                # ä¸šåŠ¡æˆæœ¬é©±åŠ¨æŸå¤±
                from models.transformer import BusinessCostLoss
                params = self.config.loss_function_params["business_cost"]
                self.criterion = BusinessCostLoss(
                    task_weights,
                    false_alarm_cost=params["false_alarm_cost"],
                    miss_change_cost=params["miss_change_cost"],
                    stable_correct_reward=params["stable_correct_reward"],
                    change_correct_reward=params["change_correct_reward"]
                )
                
            elif loss_type == "imbalanced_focal":
                # æ”¹è¿›Focal Loss
                from models.transformer import ImbalancedFocalLoss
                params = self.config.loss_function_params["imbalanced_focal"]
                self.criterion = ImbalancedFocalLoss(
                    task_weights,
                    alpha=params["alpha"],
                    gamma=params["gamma"],
                    dynamic_adjustment=params["dynamic_adjustment"]
                )
                
            else:
                raise ValueError(f"æœªçŸ¥çš„æŸå¤±å‡½æ•°ç±»å‹: {loss_type}")
                
            print(f"ğŸ“Š ä½¿ç”¨æŸå¤±å‡½æ•°: {loss_type}")
            
        else:
            # ä¸‰åˆ†ç±»æ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
            class_weights = torch.FloatTensor(
                self.config.class_weights
            ).to(self.device)

            if self.config.use_trading_aware_loss:
                # ä½¿ç”¨äº¤æ˜“æ„ŸçŸ¥æŸå¤±å‡½æ•°
                self.criterion = TradingAwareLoss(
                    self.config.task_weights,
                    self.config.direction_error_matrix,
                    class_weights
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿå¤šä»»åŠ¡æŸå¤±å‡½æ•°
                self.criterion = MultiTaskLoss(
                    self.config.task_weights, class_weights
                )

        # TensorBoard
        self.writer = SummaryWriter(self.config.log_dir)

        # æ¨¡å‹ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(
            p.numel() for p in self.model.parameters() if p.requires_grad
        )

        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡: æ€»å‚æ•°={total_params:,} | å¯è®­ç»ƒå‚æ•°={trainable_params:,} | æ¨¡å‹å¤§å°=~{total_params * 4 / 1024 / 1024:.1f}MB")
        print("âœ… æ¨¡å‹æ„å»ºå®Œæˆ")

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        correct_predictions = {
            f'{h}min': 0 for h in self.config.prediction_horizons
        }

        for batch_idx, (features, labels) in enumerate(self.train_loader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            features = features.to(self.device)
            batch_labels = {}
            for horizon, label_tensor in labels.items():
                batch_labels[horizon] = label_tensor.squeeze().to(
                    self.device
                )

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions = self.model(features)

            # è®¡ç®—æŸå¤±
            losses = self.criterion(predictions, batch_labels)
            total_loss += losses['total_loss'].item()

            # åå‘ä¼ æ’­
            losses['total_loss'].backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), max_norm=1.0
            )

            self.optimizer.step()

            # è®¡ç®—å‡†ç¡®ç‡
            batch_size = features.size(0)
            total_samples += batch_size

            for horizon in self.config.prediction_horizons:
                horizon_key = f'{horizon}min'
                pred_labels = torch.argmax(predictions[horizon_key], dim=1)
                correct = (
                    pred_labels == batch_labels[horizon_key]
                ).sum().item()
                correct_predictions[horizon_key] += correct

            # è®°å½•æ—¥å¿—
            if batch_idx % self.config.log_every_n_steps == 0:
                avg_loss = total_loss / (batch_idx + 1)
                print(
                    f"  Batch {batch_idx:4d}/{len(self.train_loader)}: "
                    f"Loss={avg_loss:.4f}"
                )

        # è®¡ç®—epochç»Ÿè®¡
        avg_loss = total_loss / len(self.train_loader)
        accuracies = {
            k: v / total_samples for k, v in correct_predictions.items()
        }
        avg_accuracy = np.mean(list(accuracies.values()))

        return avg_loss, avg_accuracy, accuracies

    def validate_epoch(self):
        """éªŒè¯ä¸€ä¸ªepoch - ä½¿ç”¨äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°"""
        self.model.eval()
        total_loss = 0.0
        
        # æ”¶é›†é¢„æµ‹ç»“æœ
        all_predictions = {f'{h}min': [] for h in self.config.prediction_horizons}
        all_labels = {f'{h}min': [] for h in self.config.prediction_horizons}
        all_probabilities = {f'{h}min': [] for h in self.config.prediction_horizons}

        with torch.no_grad():
            for features, batch_labels in self.val_loader:
                features = features.to(self.device)
                
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ ‡ç­¾ä¹Ÿç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
                labels_on_device = {}
                for horizon, label_tensor in batch_labels.items():
                    labels_on_device[horizon] = label_tensor.squeeze().to(self.device)
                
                # è®¡ç®—æŸå¤±
                predictions = self.model(features)
                batch_loss = self.criterion(predictions, labels_on_device)
                total_loss += batch_loss['total_loss'].item()

                # è·å–é¢„æµ‹æ¦‚ç‡
                probabilities = self.model.predict_proba(features)

                # æ”¶é›†é¢„æµ‹ç»“æœ
                for horizon in self.config.prediction_horizons:
                    horizon_key = f'{horizon}min'
                    pred_labels = torch.argmax(predictions[horizon_key], dim=1)
                    all_predictions[horizon_key].extend(
                        pred_labels.cpu().numpy()
                    )
                    all_labels[horizon_key].extend(
                        labels_on_device[horizon_key].cpu().numpy()
                    )
                    all_probabilities[horizon_key].extend(
                        probabilities[horizon_key].cpu().numpy()
                    )

        # è®¡ç®—äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°æŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)

        # å¯¹æ¯ä¸ªæ—¶é—´å°ºåº¦è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import (
            accuracy_score,
            confusion_matrix,
            f1_score,
            precision_score,
            recall_score,
        )
        
        evaluation_results = {}
        for horizon in self.config.prediction_horizons:
            horizon_key = f'{horizon}min'
            y_true = np.array(all_labels[horizon_key])
            y_pred = np.array(all_predictions[horizon_key])
            y_proba = np.array(all_probabilities[horizon_key])

            # è®¡ç®—åŸºç¡€åˆ†ç±»æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
            
            # è®¡ç®—æ··æ·†çŸ©é˜µç”¨äºç±»åˆ«å‡†ç¡®ç‡
            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
            
            # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡ï¼ˆå¬å›ç‡ï¼‰
            stable_accuracy = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) > 0 else 0.0  # ç¨³å®šç±»å‡†ç¡®ç‡
            change_accuracy = cm[1, 1] / (cm[1, 0] + cm[1, 1]) if (cm[1, 0] + cm[1, 1]) > 0 else 0.0  # å˜åŒ–ç±»å‡†ç¡®ç‡
            
            # ğŸ¯ è®¡ç®—ç¾éš¾æ€§é”™è¯¯ç‡ï¼ˆæ–¹å‘å®Œå…¨ç›¸åï¼‰
            catastrophic_errors = 0  # äºŒåˆ†ç±»ä¸­ï¼Œä¸¤ä¸ªç±»åˆ«äº’ç›¸è¯¯åˆ¤éƒ½ç®—ç¾éš¾æ€§é”™è¯¯
            total_samples = len(y_true)
            if total_samples > 0:
                false_positives = cm[0, 1]  # ç¨³å®šè¯¯åˆ¤ä¸ºå˜åŒ–
                false_negatives = cm[1, 0]  # å˜åŒ–è¯¯åˆ¤ä¸ºç¨³å®š
                catastrophic_errors = false_positives + false_negatives
                catastrophic_error_rate = catastrophic_errors / total_samples
            else:
                catastrophic_error_rate = 0.0
            
            # è®¡ç®—å¹³è¡¡ç±»åˆ«å‡†ç¡®ç‡
            balanced_class_accuracy = (
                self.config.class_accuracy_weights['stable_weight'] * stable_accuracy +
                self.config.class_accuracy_weights['change_weight'] * change_accuracy
            )
            
            # ğŸ¯ è®¡ç®—ç»¼åˆè¯„åˆ†
            composite_score = self._calculate_composite_score(
                balanced_class_accuracy, catastrophic_error_rate
            )
            
            # ğŸ¯ æ£€æµ‹æ¨¡å‹æ˜¯å¦å¤±è´¥
            is_failed = self._check_model_failure(
                change_accuracy, stable_accuracy, balanced_class_accuracy, catastrophic_error_rate
            )
            
            evaluation_results[horizon_key] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'stable_accuracy': stable_accuracy,
                'change_accuracy': change_accuracy,
                'balanced_class_accuracy': balanced_class_accuracy,
                'catastrophic_error_rate': catastrophic_error_rate,
                'composite_score': composite_score,
                'is_failed_model': is_failed,
                'confusion_matrix': cm,
                'probabilities': y_proba
            }

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_accuracy = np.mean([
            results['accuracy'] for results in evaluation_results.values()
        ])
        avg_precision = np.mean([
            results['precision'] for results in evaluation_results.values()
        ])
        avg_f1 = np.mean([
            results['f1_score'] for results in evaluation_results.values()
        ])
        avg_balanced_class_accuracy = np.mean([
            results['balanced_class_accuracy'] for results in evaluation_results.values()
        ])
        avg_composite_score = np.mean([
            results['composite_score'] for results in evaluation_results.values()
        ])
        avg_catastrophic_rate = np.mean([
            results['catastrophic_error_rate'] for results in evaluation_results.values()
        ])

        # è¿”å›ä¸»è¦æŒ‡æ ‡
        metrics = {
            'loss': avg_loss,
            'accuracy': avg_accuracy,
            'precision': avg_precision,
            'f1_score': avg_f1,
            'balanced_class_accuracy': avg_balanced_class_accuracy,
            'composite_score': avg_composite_score,
            'catastrophic_error_rate': avg_catastrophic_rate,
            'detailed_results': evaluation_results
        }

        return metrics

    def _calculate_composite_score(self, balanced_accuracy: float, 
                                 catastrophic_rate: float) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆå»æ‰F1åˆ†æ•°ï¼‰"""
        weights = self.config.composite_score_weights
        
        # å½’ä¸€åŒ–ç¾éš¾æ€§é”™è¯¯æ§åˆ¶åˆ†æ•°ï¼ˆé”™è¯¯ç‡è¶Šä½åˆ†æ•°è¶Šé«˜ï¼‰
        catastrophic_control_score = max(0.0, 1.0 - catastrophic_rate * 20)  # 5%é”™è¯¯ç‡å¯¹åº”0åˆ†
        
        composite_score = (
            weights['balanced_class_accuracy'] * balanced_accuracy +
            weights['catastrophic_control'] * catastrophic_control_score
        )
        
        return composite_score
    
    def _check_model_failure(self, change_accuracy: float, stable_accuracy: float,
                           balanced_accuracy: float, catastrophic_rate: float) -> bool:
        """æ£€æµ‹æ¨¡å‹æ˜¯å¦å¤±è´¥"""
        criteria = self.config.model_failure_criteria
        
        return (
            change_accuracy < criteria['min_change_accuracy'] or
            stable_accuracy < criteria['min_stable_accuracy'] or
            balanced_accuracy < criteria['min_balanced_accuracy'] or
            catastrophic_rate > criteria['max_catastrophic_rate']
        )

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*50)
        print("ğŸš€ æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
        print("="*50)

        # æ—©åœç›¸å…³å˜é‡
        best_metrics = {
            'composite_score': 0.0,
            'balanced_class_accuracy': 0.0,
            'change_accuracy': 0.0,
        }
        patience_counter = 0
        training_history = []

        for epoch in range(self.config.max_epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()

            print(f"\nğŸ“ˆ Epoch {epoch+1}/{self.config.max_epochs}")
            print("-" * 40)

            # è®­ç»ƒ
            train_loss, train_accuracy, train_accuracies = (
                self.train_epoch(epoch)
            )

            # éªŒè¯
            val_metrics = self.validate_epoch()

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()

            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_metrics['loss'])
            self.training_history['train_accuracy'].append(train_accuracy)
            self.training_history['val_accuracy'].append(
                val_metrics['accuracy']
            )
            
            # æ·»åŠ åˆ°è®­ç»ƒå†å²ï¼ˆç”¨äºå¤šæŒ‡æ ‡æ—©åœï¼‰
            training_history.append(val_metrics)

            # TensorBoardè®°å½•
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            self.writer.add_scalar(
                'Loss/Validation', val_metrics['loss'], epoch
            )
            self.writer.add_scalar('Accuracy/Train', train_accuracy, epoch)
            self.writer.add_scalar(
                'Accuracy/Validation',
                val_metrics['accuracy'], epoch
            )
            
            # è®°å½•æ–°çš„å…³é”®æŒ‡æ ‡
            self.writer.add_scalar('Metrics/Composite_Score', val_metrics['composite_score'], epoch)
            self.writer.add_scalar('Metrics/Balanced_Class_Accuracy', val_metrics['balanced_class_accuracy'], epoch)
            self.writer.add_scalar('Metrics/Catastrophic_Error_Rate', val_metrics['catastrophic_error_rate'], epoch)

            # æ‰“å°è®­ç»ƒç»“æœ
            epoch_time = time.time() - epoch_start_time
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.3f}")
            print(f"éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")

            # æ‰“å°äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°ç»“æœ
            print(f"\nğŸ¯ äº¤æ˜“æ„ŸçŸ¥è¯„ä¼°: ç»¼åˆè¯„åˆ†={val_metrics['composite_score']:.3f} | "
                  f"å¹³è¡¡å‡†ç¡®ç‡={val_metrics['balanced_class_accuracy']:.3f} | "
                  f"ç¾éš¾é”™è¯¯ç‡={val_metrics['catastrophic_error_rate']:.3f}")

            for horizon, results in val_metrics['detailed_results'].items():
                failed_status = "âŒå¤±è´¥" if results['is_failed_model'] else "âœ…æ­£å¸¸"
                cm = results['confusion_matrix']
                print(f"{horizon}: {failed_status} | ç¨³å®šç±»={results['stable_accuracy']:.3f}, å˜åŒ–ç±»={results['change_accuracy']:.3f}, ç»¼åˆè¯„åˆ†={results['composite_score']:.3f} | æ··æ·†çŸ©é˜µ{cm.tolist()}")

            print(f"è€—æ—¶: {epoch_time:.2f}s")

            # å¤šé‡æ¨¡å‹ä¿å­˜é€»è¾‘
            model_improved = False
            
            # 1. ç»¼åˆè¯„åˆ†æœ€ä½³æ¨¡å‹
            if val_metrics['composite_score'] > best_metrics['composite_score']:
                best_metrics['composite_score'] = val_metrics['composite_score']
                self.save_model('best_composite.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³ç»¼åˆè¯„åˆ†æ¨¡å‹")
                model_improved = True
                
            # 2. å¹³è¡¡å‡†ç¡®ç‡æœ€ä½³æ¨¡å‹
            if val_metrics['balanced_class_accuracy'] > best_metrics['balanced_class_accuracy']:
                best_metrics['balanced_class_accuracy'] = val_metrics['balanced_class_accuracy']
                self.save_model('best_balanced.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³å¹³è¡¡å‡†ç¡®ç‡æ¨¡å‹")
                
            # 3. å˜åŒ–ç±»å‡†ç¡®ç‡æœ€ä½³æ¨¡å‹
            for horizon, results in val_metrics['detailed_results'].items():
                if results['change_accuracy'] > best_metrics['change_accuracy']:
                    best_metrics['change_accuracy'] = results['change_accuracy']
                    self.save_model('best_change.pth')
                    print("ğŸ’¾ ä¿å­˜æœ€ä½³å˜åŒ–ç±»å‡†ç¡®ç‡æ¨¡å‹")

            # ğŸ¯ æ”¹è¿›çš„æ—©åœåˆ¤æ–­
            should_stop, stop_reason = self._should_stop_training(
                training_history, patience_counter
            )
            
            if model_improved:
                patience_counter = 0
            else:
                patience_counter += 1
                
            if should_stop:
                print(f"ğŸ›‘ æ—©åœè§¦å‘: {stop_reason}")
                break

            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.config.save_every_n_epochs == 0:
                self.save_model(f'checkpoint_epoch_{epoch+1}.pth')

        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³ç»¼åˆè¯„åˆ†: {best_metrics['composite_score']:.3f}")
        print(f"æœ€ä½³å¹³è¡¡å‡†ç¡®ç‡: {best_metrics['balanced_class_accuracy']:.3f}")
        print(f"æœ€ä½³å˜åŒ–ç±»å‡†ç¡®ç‡: {best_metrics['change_accuracy']:.3f}")

        # è¾“å‡ºæ‰€æœ‰è½®è®­ç»ƒçš„è¯¦ç»†å†å²
        print("\nğŸ“ˆ è®­ç»ƒå†å²å›é¡¾:")
        print("=" * 80)
        
        # è·å–å‚æ•°ç»„åˆä¿¡æ¯
        if (self.config.loss_function_type == "business_cost" and 
            "business_cost" in self.config.loss_function_params):
            bc_params = self.config.loss_function_params["business_cost"]
            false_alarm = bc_params.get("false_alarm_cost", "N/A")
            miss_change = bc_params.get("miss_change_cost", "N/A")
            stable_correct_reward = bc_params.get("stable_correct_reward", "N/A")
            change_correct_reward = bc_params.get("change_correct_reward", "N/A")
            param_info = f"å‚æ•°[è¯¯æŠ¥={false_alarm}, æ¼æŠ¥={miss_change}, å¥–åŠ±=[ç¨³å®š={stable_correct_reward}, å˜åŒ–={change_correct_reward}]"
        else:
            param_info = "å‚æ•°[æ ‡å‡†é…ç½®]"
        
        print(f"é…ç½®: {param_info}")
        
        for epoch, metrics in enumerate(training_history, 1):
            for horizon, results in metrics['detailed_results'].items():
                failed_status = "âŒå¤±è´¥" if results['is_failed_model'] else "âœ…æ­£å¸¸"
                cm = results['confusion_matrix']
                print(f"Epoch{epoch} {horizon}: {failed_status} | ç¨³å®šç±»={results['stable_accuracy']:.3f}, å˜åŒ–ç±»={results['change_accuracy']:.3f}, ç»¼åˆè¯„åˆ†={results['composite_score']:.3f} | æ··æ·†çŸ©é˜µ{cm.tolist()}")
        print("=" * 80)

    def _should_stop_training(self, history: list, patience_counter: int) -> tuple:
        """æ”¹è¿›çš„æ—©åœåˆ¤æ–­é€»è¾‘"""
        if len(history) < 5:  # è‡³å°‘è®­ç»ƒ5è½®
            return False, ""
            
        # åŸºç¡€è€å¿ƒæ£€æŸ¥
        if patience_counter >= self.config.early_stopping_patience:
            return True, f"è¿ç»­{patience_counter}è½®ç»¼åˆè¯„åˆ†æœªæå‡"
        
        # å¤šæŒ‡æ ‡æ—©åœæ£€æŸ¥
        if self.config.multi_metric_early_stopping['enabled']:
            window = self.config.multi_metric_early_stopping['stable_trend_window']
            if len(history) >= window:
                recent_metrics = history[-window:]
                
                # æ£€æŸ¥å˜åŒ–ç±»å‡†ç¡®ç‡æ˜¯å¦ä¸¥é‡ä¸‹é™
                change_accuracies = []
                for metrics in recent_metrics:
                    for results in metrics['detailed_results'].values():
                        change_accuracies.append(results['change_accuracy'])
                
                if len(change_accuracies) >= 2:
                    change_trend = change_accuracies[-1] - change_accuracies[0]
                    decline_limit = self.config.multi_metric_early_stopping['change_accuracy_decline_limit']
                    
                    if change_trend < decline_limit:
                        return True, f"å˜åŒ–ç±»å‡†ç¡®ç‡ä¸‹é™è¿‡å¤š: {change_trend:.3f}"
                
                # æ£€æŸ¥ç»¼åˆè¯„åˆ†æ˜¯å¦åœæ»
                composite_scores = [m['composite_score'] for m in recent_metrics]
                if len(composite_scores) >= 2:
                    score_improvement = max(composite_scores) - min(composite_scores)
                    min_improvement = self.config.multi_metric_early_stopping['min_improvement_threshold']
                    
                    if score_improvement < min_improvement and patience_counter >= 8:
                        return True, f"ç»¼åˆè¯„åˆ†æ”¹å–„åœæ»: {score_improvement:.4f} < {min_improvement}"
        
        return False, ""

    def evaluate(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
        print("\n" + "="*50)
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°é˜¶æ®µ")
        print("="*50)

        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(
            self.config.model_save_dir, 'best_model.pth'
        )
        if os.path.exists(best_model_path):
            self.model.load_state_dict(torch.load(
                best_model_path, map_location=self.device
            ))
            print("ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹")

        self.model.eval()
        all_predictions = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }
        all_labels = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }
        all_probabilities = {
            f'{h}min': [] for h in self.config.prediction_horizons
        }

        with torch.no_grad():
            for features, labels in self.test_loader:
                features = features.to(self.device)

                # é¢„æµ‹
                predictions = self.model(features)
                probabilities = self.model.predict_proba(features)

                for horizon in self.config.prediction_horizons:
                    horizon_key = f'{horizon}min'

                    # é¢„æµ‹æ ‡ç­¾
                    pred_labels = torch.argmax(
                        predictions[horizon_key], dim=1
                    )
                    all_predictions[horizon_key].extend(
                        pred_labels.cpu().numpy()
                    )

                    # çœŸå®æ ‡ç­¾
                    true_labels = labels[horizon_key].squeeze()
                    all_labels[horizon_key].extend(true_labels.numpy())

                    # é¢„æµ‹æ¦‚ç‡
                    probs = probabilities[horizon_key]
                    all_probabilities[horizon_key].extend(
                        probs.cpu().numpy()
                    )

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        from sklearn.metrics import (
            accuracy_score,
            confusion_matrix,
            precision_recall_fscore_support,
        )

        results = {}
        for horizon in self.config.prediction_horizons:
            horizon_key = f'{horizon}min'

            y_true = np.array(all_labels[horizon_key])
            y_pred = np.array(all_predictions[horizon_key])
            y_proba = np.array(all_probabilities[horizon_key])

            # åŸºç¡€æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            (precision, recall,
             f1, _) = precision_recall_fscore_support(
                y_true, y_pred, average='weighted'
            )
            cm = confusion_matrix(y_true, y_pred)

            # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
            tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]
            stable_accuracy = tn / (tn + fp) if (tn + fp) > 0 else 0.0  # ç¨³å®šç±»å‡†ç¡®ç‡
            change_accuracy = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # å˜åŒ–ç±»å‡†ç¡®ç‡

            results[horizon_key] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'confusion_matrix': cm,
                'probabilities': y_proba,
                'stable_accuracy': stable_accuracy,
                'change_accuracy': change_accuracy,
            }

            print(f"\n{horizon_key} é¢„æµ‹ç»“æœ: å‡†ç¡®ç‡={accuracy:.3f} | "
                  f"ç²¾ç¡®ç‡={precision:.3f} | å¬å›ç‡={recall:.3f} | F1={f1:.3f}")
            print(f"  æ··æ·†çŸ©é˜µ: {cm.tolist()}")
            print(f"  ç¨³å®šç±»å‡†ç¡®ç‡: {stable_accuracy:.3f} | å˜åŒ–ç±»å‡†ç¡®ç‡: {change_accuracy:.3f}")

        return results

    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.config.model_save_dir, filename)
        torch.save(self.model.state_dict(), model_path)

    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.config.plot_training_curves:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.training_history['train_loss'], label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(self.training_history['val_loss'], label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('æŸå¤±æ›²çº¿')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.training_history['train_accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
        axes[0, 1].plot(self.training_history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        plt.tight_layout()
        plot_path = os.path.join(self.config.plot_dir, 'training_curves.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")

    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        try:
            print("ğŸ§ª å¼€å§‹æ·±åº¦å­¦ä¹ å®éªŒ")
            print("=" * 60)

            # 1. æ•°æ®å‡†å¤‡
            self.prepare_data()

            # 2. æ¨¡å‹æ„å»º
            self.build_model()

            # 3. æ¨¡å‹è®­ç»ƒ
            self.train()

            # 4. æ¨¡å‹è¯„ä¼°
            results = self.evaluate()

            # 5. å¯è§†åŒ–
            self.plot_training_curves()

            print("\n" + "=" * 60)
            print("ğŸ‰ å®éªŒå®Œæˆï¼")
            print("=" * 60)

            return results

        except Exception as e:
            print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
            raise
        finally:
            if self.writer:
                self.writer.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦å­¦ä¹ å®éªŒç¨‹åº')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data', type=str, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    config = ExperimentConfig()

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.data:
        config.data_file = args.data
    if args.epochs:
        config.max_epochs = args.epochs
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.lr:
        config.learning_rate = args.lr

    # è¿è¡Œå®éªŒ
    experiment = DeepLearningExperiment(config)
    results = experiment.run_experiment()

    return results


if __name__ == "__main__":
    main()
    main()
