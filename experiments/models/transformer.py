"""
ğŸ§ª Transformeræ¨¡å‹æ¶æ„
ç‹¬ç«‹å®éªŒç¨‹åºï¼Œä¸“ä¸ºKçº¿æ—¶åºé¢„æµ‹è®¾è®¡
"""

import math
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    
    def __init__(self, d_model: int, max_seq_length: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                                   mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        output = torch.matmul(attention_weights, V)
        return output, attention_weights
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢å’Œé‡å¡‘
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # é‡å¡‘å’Œè¾“å‡ºå˜æ¢
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        output = self.W_o(attention_output)
        return output, attention_weights

class FeedForward(nn.Module):
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class TransformerBlock(nn.Module):
    """å•ä¸ªTransformerå—"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class MarketTransformer(nn.Module):
    """
    å¸‚åœºæ—¶åºé¢„æµ‹Transformeræ¨¡å‹
    
    ä¸“ä¸ºKçº¿æ•°æ®è®¾è®¡çš„Transformeræ¶æ„ï¼Œæ”¯æŒå¤šæ—¶é—´å°ºåº¦é¢„æµ‹
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # é…ç½®å‚æ•°
        self.feature_dim = config['feature_dim']
        self.embed_dim = config['embed_dim']
        self.num_heads = config['num_heads']
        self.num_layers = config['num_layers']
        self.ff_dim = config['ff_dim']
        self.dropout = config['dropout']
        self.sequence_length = config['sequence_length']
        self.prediction_horizons = config['prediction_horizons']
        # æ”¯æŒäºŒåˆ†ç±»å’Œä¸‰åˆ†ç±»
        self.num_classes = config.get('num_classes', 2)  # é»˜è®¤ä¸º2ï¼ˆäºŒåˆ†ç±»ï¼‰
        
        # è¾“å…¥åµŒå…¥å±‚
        self.input_embedding = nn.Linear(self.feature_dim, self.embed_dim)
        self.input_norm = nn.LayerNorm(self.embed_dim)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(self.embed_dim, self.sequence_length)
        
        # Transformerå±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.dropout)
            for _ in range(self.num_layers)
        ])
        
        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # é¢„æµ‹å¤´ï¼ˆæ¯ä¸ªæ—¶é—´å°ºåº¦ä¸€ä¸ªï¼‰
        self.prediction_heads = nn.ModuleDict()
        for horizon in self.prediction_horizons:
            self.prediction_heads[f'{horizon}min'] = nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 2, self.embed_dim // 4),
                nn.ReLU(), 
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 4, self.num_classes)
            )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def create_padding_mask(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """åˆ›å»ºpadding mask"""
        if lengths is None:
            # å¦‚æœæ²¡æœ‰æä¾›é•¿åº¦ï¼Œå‡è®¾æ‰€æœ‰åºåˆ—éƒ½æ˜¯å®Œæ•´çš„
            return None
        
        batch_size, seq_len = x.size(0), x.size(1)
        mask = torch.arange(seq_len, device=x.device).expand(
            batch_size, seq_len) < lengths.unsqueeze(1)
        return mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, sequence_length, feature_dim]
            lengths: åºåˆ—å®é™…é•¿åº¦ [batch_size] (å¯é€‰)
            
        Returns:
            æ¯ä¸ªæ—¶é—´å°ºåº¦çš„é¢„æµ‹ç»“æœå­—å…¸
        """
        batch_size, seq_len, feature_dim = x.size()
        
        # è¾“å…¥åµŒå…¥å’Œå½’ä¸€åŒ–
        x = self.input_embedding(x)  # [batch, seq_len, embed_dim]
        x = self.input_norm(x)
        
        # ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # [seq_len, batch, embed_dim]
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # [batch, seq_len, embed_dim]
        
        # åˆ›å»ºmask
        mask = self.create_padding_mask(x, lengths)
        
        # Transformerå±‚
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x, mask)
        
        # å…¨å±€æ± åŒ–ï¼šå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        # æˆ–è€…å¯ä»¥ä½¿ç”¨å¹³å‡æ± åŒ–ï¼šx.mean(dim=1)
        pooled_output = x[:, -1, :]  # [batch, embed_dim]
        
        # å¤šä»»åŠ¡é¢„æµ‹
        predictions = {}
        for horizon in self.prediction_horizons:
            pred_logits = self.prediction_heads[f'{horizon}min'](pooled_output)
            predictions[f'{horizon}min'] = pred_logits
        
        return predictions
    
    def predict_proba(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x, lengths)
            probabilities = {}
            for horizon, logit in logits.items():
                probabilities[horizon] = F.softmax(logit, dim=-1)
        return probabilities
    
    def get_attention_weights(self, x: torch.Tensor, layer_idx: int = -1) -> torch.Tensor:
        """è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        self.eval()
        with torch.no_grad():
            batch_size, seq_len, feature_dim = x.size()
            
            # å‰å‘ä¼ æ’­åˆ°æŒ‡å®šå±‚
            x = self.input_embedding(x)
            x = self.input_norm(x)
            x = x.transpose(0, 1)
            x = self.positional_encoding(x)
            x = x.transpose(0, 1)
            
            target_layer = self.transformer_blocks[layer_idx]
            attention_output, attention_weights = target_layer.attention(x, x, x)
            
            return attention_weights

class BinaryClassificationLoss(nn.Module):
    """ç®€åŒ–çš„äºŒåˆ†ç±»æŸå¤±å‡½æ•° - ä¸“ä¸ºæ¶¨è·Œé¢„æµ‹è®¾è®¡"""
    
    def __init__(self, task_weights: Dict[str, float], 
                 class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.task_weights = task_weights
        self.class_weights = class_weights
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—äºŒåˆ†ç±»æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ {horizon: [batch_size, 2]}
            targets: çœŸå®æ ‡ç­¾ {horizon: [batch_size]} (0=æ¶¨, 1=è·Œ)
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            
            # ğŸ”§ è¾“å…¥éªŒè¯
            if pred_logits.size(0) != target_labels.size(0):
                raise ValueError(f"é¢„æµ‹å’Œæ ‡ç­¾æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: {pred_logits.size(0)} vs {target_labels.size(0)}")
            
            if pred_logits.size(1) != 2:
                raise ValueError(f"äºŒåˆ†ç±»è¦æ±‚é¢„æµ‹ç»´åº¦ä¸º2ï¼Œä½†å¾—åˆ°{pred_logits.size(1)}")
            
            # æ£€æŸ¥æ ‡ç­¾å€¼åŸŸï¼ˆåº”è¯¥åªæœ‰0å’Œ1ï¼‰
            unique_labels = torch.unique(target_labels)
            if not all(label.item() in [0, 1] for label in unique_labels):
                raise ValueError(f"äºŒåˆ†ç±»æ ‡ç­¾åº”è¯¥åªåŒ…å«0å’Œ1ï¼Œä½†å¾—åˆ°{unique_labels.tolist()}")
            
            # è®¡ç®—äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
            task_loss = F.cross_entropy(pred_logits, target_labels, 
                                      weight=self.class_weights)
            
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses

class TradingAwareLoss(nn.Module):
    """äº¤æ˜“æ„ŸçŸ¥æŸå¤±å‡½æ•° - é‡ç‚¹æƒ©ç½šæ–¹å‘æ€§é”™è¯¯"""
    
    def __init__(self, task_weights: Dict[str, float], 
                 direction_error_matrix: List[List[float]], 
                 class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.task_weights = task_weights
        self.class_weights = class_weights
        
        # è½¬æ¢é”™è¯¯æƒé‡çŸ©é˜µä¸ºtensor
        self.error_matrix = torch.tensor(direction_error_matrix, dtype=torch.float32)
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—äº¤æ˜“æ„ŸçŸ¥æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            
            # åŸºç¡€äº¤å‰ç†µæŸå¤±
            base_loss = F.cross_entropy(pred_logits, target_labels, 
                                      weight=self.class_weights, reduction='none')
            
            # åæŒå¹³åå¥½ï¼šé¢å¤–æƒ©ç½šé¢„æµ‹æŒå¹³ç±»
            pred_probs = F.softmax(pred_logits, dim=1)
            flat_penalty = pred_probs[:, 2] * 0.5  # å¯¹æŒå¹³é¢„æµ‹æ¦‚ç‡é¢å¤–æƒ©ç½š
            
            # æ–¹å‘æ€§å¥–åŠ±ï¼šå¥–åŠ±æ¶¨è·Œé¢„æµ‹
            directional_reward = (pred_probs[:, 0] + pred_probs[:, 1]) * 0.2
            
            # ç»¼åˆæŸå¤±
            adjusted_loss = base_loss + flat_penalty - directional_reward
            
            # è®¡ç®—æ–¹å‘æ€§é”™è¯¯æƒ©ç½š
            pred_classes = torch.argmax(pred_logits, dim=1)
            
            # ç¡®ä¿é”™è¯¯æƒé‡çŸ©é˜µåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            error_matrix = self.error_matrix.to(pred_logits.device)
            
            # è·å–æ¯ä¸ªæ ·æœ¬çš„é”™è¯¯æƒé‡
            error_weights = error_matrix[target_labels, pred_classes]
            
            # åº”ç”¨æ–¹å‘æ€§é”™è¯¯æƒ©ç½š
            directional_loss = adjusted_loss * (1.0 + error_weights * 0.1)
            task_loss = directional_loss.mean()
            
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses

class MultiTaskLoss(nn.Module):
    """ä¼ ç»Ÿå¤šä»»åŠ¡æŸå¤±å‡½æ•°ï¼ˆä¿ç•™ç”¨äºå¯¹æ¯”ï¼‰"""
    
    def __init__(self, task_weights: Dict[str, float], class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.task_weights = task_weights
        self.class_weights = class_weights
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å¤šä»»åŠ¡æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            task_loss = F.cross_entropy(pred_logits, target_labels, weight=self.class_weights)
            
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses

def create_model(config: Dict) -> MarketTransformer:
    """
    åˆ›å»ºæ¨¡å‹å·¥å‚å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å‹
    """
    return MarketTransformer(config)

def test_transformer_model():
    """æµ‹è¯•Transformeræ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•Transformeræ¨¡å‹...")
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'feature_dim': 38,
        'embed_dim': 256,
        'num_heads': 8,
        'num_layers': 6,
        'ff_dim': 1024,
        'dropout': 0.1,
        'sequence_length': 30,
        'prediction_horizons': [5, 10, 15]
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = MarketTransformer(config)
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 32
    seq_length = 30
    feature_dim = 38
    
    x = torch.randn(batch_size, seq_length, feature_dim)
    
    # å‰å‘ä¼ æ’­
    outputs = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print("è¾“å‡ºå½¢çŠ¶:")
    for horizon, output in outputs.items():
        print(f"  {horizon}: {output.shape}")
    
    # æµ‹è¯•æ¦‚ç‡é¢„æµ‹
    probas = model.predict_proba(x)
    print("\næ¦‚ç‡é¢„æµ‹:")
    for horizon, proba in probas.items():
        print(f"  {horizon}: {proba.shape}, sum={proba.sum(dim=1)[0]:.3f}")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\næ¨¡å‹å‚æ•°:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model

class TwoTaskTransformer(nn.Module):
    """
    ä¸¤ä»»åŠ¡Transformeræ¨¡å‹ - ç¨³å®šæ€§æ£€æµ‹ + æ–¹å‘åˆ¤æ–­
    
    ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹ (stable vs unstable)
    ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­ (up vs down, ä»…åœ¨unstableæ—¶)
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # é…ç½®å‚æ•°
        self.feature_dim = config['feature_dim']
        self.embed_dim = config['embed_dim']
        self.num_heads = config['num_heads']
        self.num_layers = config['num_layers']
        self.ff_dim = config['ff_dim']
        self.dropout = config['dropout']
        self.sequence_length = config['sequence_length']
        self.prediction_horizons = config['prediction_horizons']
        
        # è¾“å…¥åµŒå…¥å±‚
        self.input_embedding = nn.Linear(self.feature_dim, self.embed_dim)
        self.input_norm = nn.LayerNorm(self.embed_dim)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(self.embed_dim, self.sequence_length)
        
        # å…±äº«çš„Transformerå±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.dropout)
            for _ in range(self.num_layers)
        ])
        
        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹å¤´ï¼ˆæ¯ä¸ªæ—¶é—´å°ºåº¦ä¸€ä¸ªï¼‰
        self.stability_heads = nn.ModuleDict()
        for horizon in self.prediction_horizons:
            self.stability_heads[f'{horizon}min'] = nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 2, 2)  # stable=0, unstable=1
            )
        
        # ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­å¤´ï¼ˆæ¯ä¸ªæ—¶é—´å°ºåº¦ä¸€ä¸ªï¼‰
        self.direction_heads = nn.ModuleDict()
        for horizon in self.prediction_horizons:
            self.direction_heads[f'{horizon}min'] = nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 2, 2)  # up=0, down=1
            )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, sequence_length, feature_dim]
            lengths: åºåˆ—å®é™…é•¿åº¦ [batch_size] (å¯é€‰)
            
        Returns:
            ä¸¤ä¸ªä»»åŠ¡çš„é¢„æµ‹ç»“æœå­—å…¸
        """
        batch_size, seq_len, feature_dim = x.size()
        
        # è¾“å…¥åµŒå…¥å’Œå½’ä¸€åŒ–
        x = self.input_embedding(x)  # [batch, seq_len, embed_dim]
        x = self.input_norm(x)
        
        # ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # [seq_len, batch, embed_dim]
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # [batch, seq_len, embed_dim]
        
        # Transformerå±‚
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        # å…¨å±€æ± åŒ–ï¼šå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        pooled_output = x[:, -1, :]  # [batch, embed_dim]
        
        # å¤šä»»åŠ¡é¢„æµ‹
        predictions = {}
        
        # ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹
        for horizon in self.prediction_horizons:
            stability_logits = self.stability_heads[f'{horizon}min'](pooled_output)
            predictions[f'stability_{horizon}min'] = stability_logits
        
        # ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­
        for horizon in self.prediction_horizons:
            direction_logits = self.direction_heads[f'{horizon}min'](pooled_output)
            predictions[f'direction_{horizon}min'] = direction_logits
        
        return predictions


class TwoTaskLoss(nn.Module):
    """ä¸¤ä»»åŠ¡æŸå¤±å‡½æ•°"""
    
    def __init__(self, task_weights: Dict[str, float], 
                 stability_weight: float = 0.4,
                 direction_weight: float = 0.6):
        super().__init__()
        self.task_weights = task_weights
        self.stability_weight = stability_weight  # ç¨³å®šæ€§ä»»åŠ¡æƒé‡
        self.direction_weight = direction_weight  # æ–¹å‘ä»»åŠ¡æƒé‡
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—ä¸¤ä»»åŠ¡æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        # ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹æŸå¤±
        stability_loss = 0.0
        stability_count = 0
        
        for task_name, pred_logits in predictions.items():
            if task_name.startswith('stability_'):
                horizon_key = task_name.replace('stability_', '')
                if task_name in targets:
                    target_labels = targets[task_name]
                    task_loss = F.cross_entropy(pred_logits, target_labels)
                    
                    stability_loss += task_loss * self.task_weights.get(horizon_key, 1.0)
                    stability_count += 1
                    losses[f'loss_{task_name}'] = task_loss
        
        if stability_count > 0:
            stability_loss /= stability_count
            total_loss += self.stability_weight * stability_loss
            losses['stability_loss'] = stability_loss
        
        # ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­æŸå¤±ï¼ˆåªåœ¨æœ‰æ•ˆæ ·æœ¬ä¸Šè®¡ç®—ï¼‰
        direction_loss = 0.0
        direction_count = 0
        
        for task_name, pred_logits in predictions.items():
            if task_name.startswith('direction_'):
                horizon_key = task_name.replace('direction_', '')
                if task_name in targets:
                    target_labels = targets[task_name]
                    
                    # è¿‡æ»¤æ‰æ— æ•ˆæ ‡ç­¾ï¼ˆ-1ï¼‰
                    valid_mask = target_labels != -1
                    if valid_mask.sum() > 0:
                        valid_pred = pred_logits[valid_mask]
                        valid_target = target_labels[valid_mask]
                        
                        task_loss = F.cross_entropy(valid_pred, valid_target)
                        direction_loss += task_loss * self.task_weights.get(horizon_key, 1.0)
                        direction_count += 1
                        losses[f'loss_{task_name}'] = task_loss
        
        if direction_count > 0:
            direction_loss /= direction_count
            total_loss += self.direction_weight * direction_loss
            losses['direction_loss'] = direction_loss
        
        losses['total_loss'] = total_loss
        return losses

if __name__ == "__main__":
    test_transformer_model() 