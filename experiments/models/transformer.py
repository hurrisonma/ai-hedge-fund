"""
ğŸ§ª Transformeræ¨¡å‹æ¶æ„
ç‹¬ç«‹å®éªŒç¨‹åºï¼Œä¸“ä¸ºKçº¿æ—¶åºé¢„æµ‹è®¾è®¡
"""

import math
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    
    def __init__(self, d_model: int, max_seq_length: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                                   mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        output = torch.matmul(attention_weights, V)
        return output, attention_weights
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢å’Œé‡å¡‘
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # é‡å¡‘å’Œè¾“å‡ºå˜æ¢
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        output = self.W_o(attention_output)
        return output, attention_weights

class FeedForward(nn.Module):
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class TransformerBlock(nn.Module):
    """å•ä¸ªTransformerå—"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class MarketTransformer(nn.Module):
    """
    å¸‚åœºæ—¶åºé¢„æµ‹Transformeræ¨¡å‹
    
    ä¸“ä¸ºKçº¿æ•°æ®è®¾è®¡çš„Transformeræ¶æ„ï¼Œæ”¯æŒå¤šæ—¶é—´å°ºåº¦é¢„æµ‹
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # é…ç½®å‚æ•°
        self.feature_dim = config['feature_dim']
        self.embed_dim = config['embed_dim']
        self.num_heads = config['num_heads']
        self.num_layers = config['num_layers']
        self.ff_dim = config['ff_dim']
        self.dropout = config['dropout']
        self.sequence_length = config['sequence_length']
        self.prediction_horizons = config['prediction_horizons']
        # æ”¯æŒäºŒåˆ†ç±»å’Œä¸‰åˆ†ç±»
        self.num_classes = config.get('num_classes', 2)  # é»˜è®¤ä¸º2ï¼ˆäºŒåˆ†ç±»ï¼‰
        
        # è¾“å…¥åµŒå…¥å±‚
        self.input_embedding = nn.Linear(self.feature_dim, self.embed_dim)
        self.input_norm = nn.LayerNorm(self.embed_dim)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(self.embed_dim, self.sequence_length)
        
        # Transformerå±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.dropout)
            for _ in range(self.num_layers)
        ])
        
        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # é¢„æµ‹å¤´ï¼ˆæ¯ä¸ªæ—¶é—´å°ºåº¦ä¸€ä¸ªï¼‰
        self.prediction_heads = nn.ModuleDict()
        for horizon in self.prediction_horizons:
            self.prediction_heads[f'{horizon}min'] = nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 2, self.embed_dim // 4),
                nn.ReLU(), 
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 4, self.num_classes)
            )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def create_padding_mask(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """åˆ›å»ºpadding mask"""
        if lengths is None:
            # å¦‚æœæ²¡æœ‰æä¾›é•¿åº¦ï¼Œå‡è®¾æ‰€æœ‰åºåˆ—éƒ½æ˜¯å®Œæ•´çš„
            return None
        
        batch_size, seq_len = x.size(0), x.size(1)
        mask = torch.arange(seq_len, device=x.device).expand(
            batch_size, seq_len) < lengths.unsqueeze(1)
        return mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, sequence_length, feature_dim]
            lengths: åºåˆ—å®é™…é•¿åº¦ [batch_size] (å¯é€‰)
            
        Returns:
            æ¯ä¸ªæ—¶é—´å°ºåº¦çš„é¢„æµ‹ç»“æœå­—å…¸
        """
        batch_size, seq_len, feature_dim = x.size()
        
        # è¾“å…¥åµŒå…¥å’Œå½’ä¸€åŒ–
        x = self.input_embedding(x)  # [batch, seq_len, embed_dim]
        x = self.input_norm(x)
        
        # ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # [seq_len, batch, embed_dim]
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # [batch, seq_len, embed_dim]
        
        # åˆ›å»ºmask
        mask = self.create_padding_mask(x, lengths)
        
        # Transformerå±‚
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x, mask)
        
        # å…¨å±€æ± åŒ–ï¼šå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        # æˆ–è€…å¯ä»¥ä½¿ç”¨å¹³å‡æ± åŒ–ï¼šx.mean(dim=1)
        pooled_output = x[:, -1, :]  # [batch, embed_dim]
        
        # å¤šä»»åŠ¡é¢„æµ‹
        predictions = {}
        for horizon in self.prediction_horizons:
            pred_logits = self.prediction_heads[f'{horizon}min'](pooled_output)
            predictions[f'{horizon}min'] = pred_logits
        
        return predictions
    
    def predict_proba(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"""
        self.eval()
        with torch.no_grad():
            logits = self.forward(x, lengths)
            probabilities = {}
            for horizon, logit in logits.items():
                probabilities[horizon] = F.softmax(logit, dim=-1)
        return probabilities
    
    def get_attention_weights(self, x: torch.Tensor, layer_idx: int = -1) -> torch.Tensor:
        """è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        self.eval()
        with torch.no_grad():
            batch_size, seq_len, feature_dim = x.size()
            
            # å‰å‘ä¼ æ’­åˆ°æŒ‡å®šå±‚
            x = self.input_embedding(x)
            x = self.input_norm(x)
            x = x.transpose(0, 1)
            x = self.positional_encoding(x)
            x = x.transpose(0, 1)
            
            target_layer = self.transformer_blocks[layer_idx]
            attention_output, attention_weights = target_layer.attention(x, x, x)
            
            return attention_weights

class BinaryClassificationLoss(nn.Module):
    """ç®€åŒ–çš„äºŒåˆ†ç±»æŸå¤±å‡½æ•° - ä¸“ä¸ºæ¶¨è·Œé¢„æµ‹è®¾è®¡"""
    
    def __init__(self, task_weights: Dict[str, float], 
                 class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.task_weights = task_weights
        self.class_weights = class_weights
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—äºŒåˆ†ç±»æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ {horizon: [batch_size, 2]}
            targets: çœŸå®æ ‡ç­¾ {horizon: [batch_size]} (0=æ¶¨, 1=è·Œ)
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            
            # ğŸ”§ è¾“å…¥éªŒè¯
            if pred_logits.size(0) != target_labels.size(0):
                raise ValueError(f"é¢„æµ‹å’Œæ ‡ç­¾æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: {pred_logits.size(0)} vs {target_labels.size(0)}")
            
            if pred_logits.size(1) != 2:
                raise ValueError(f"äºŒåˆ†ç±»è¦æ±‚é¢„æµ‹ç»´åº¦ä¸º2ï¼Œä½†å¾—åˆ°{pred_logits.size(1)}")
            
            # æ£€æŸ¥æ ‡ç­¾å€¼åŸŸï¼ˆåº”è¯¥åªæœ‰0å’Œ1ï¼‰
            unique_labels = torch.unique(target_labels)
            if not all(label.item() in [0, 1] for label in unique_labels):
                raise ValueError(f"äºŒåˆ†ç±»æ ‡ç­¾åº”è¯¥åªåŒ…å«0å’Œ1ï¼Œä½†å¾—åˆ°{unique_labels.tolist()}")
            
            # è®¡ç®—äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
            task_loss = F.cross_entropy(pred_logits, target_labels, 
                                      weight=self.class_weights)
            
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses

class TradingAwareLoss(nn.Module):
    """äº¤æ˜“æ„ŸçŸ¥æŸå¤±å‡½æ•° - é‡ç‚¹æƒ©ç½šæ–¹å‘æ€§é”™è¯¯"""
    
    def __init__(self, task_weights: Dict[str, float], 
                 direction_error_matrix: List[List[float]], 
                 class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.task_weights = task_weights
        self.class_weights = class_weights
        
        # è½¬æ¢é”™è¯¯æƒé‡çŸ©é˜µä¸ºtensor
        self.error_matrix = torch.tensor(direction_error_matrix, dtype=torch.float32)
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—äº¤æ˜“æ„ŸçŸ¥æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            
            # åŸºç¡€äº¤å‰ç†µæŸå¤±
            base_loss = F.cross_entropy(pred_logits, target_labels, 
                                      weight=self.class_weights, reduction='none')
            
            # åæŒå¹³åå¥½ï¼šé¢å¤–æƒ©ç½šé¢„æµ‹æŒå¹³ç±»
            pred_probs = F.softmax(pred_logits, dim=1)
            flat_penalty = pred_probs[:, 2] * 0.5  # å¯¹æŒå¹³é¢„æµ‹æ¦‚ç‡é¢å¤–æƒ©ç½š
            
            # æ–¹å‘æ€§å¥–åŠ±ï¼šå¥–åŠ±æ¶¨è·Œé¢„æµ‹
            directional_reward = (pred_probs[:, 0] + pred_probs[:, 1]) * 0.2
            
            # ç»¼åˆæŸå¤±
            adjusted_loss = base_loss + flat_penalty - directional_reward
            
            # è®¡ç®—æ–¹å‘æ€§é”™è¯¯æƒ©ç½š
            pred_classes = torch.argmax(pred_logits, dim=1)
            
            # ç¡®ä¿é”™è¯¯æƒé‡çŸ©é˜µåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            error_matrix = self.error_matrix.to(pred_logits.device)
            
            # è·å–æ¯ä¸ªæ ·æœ¬çš„é”™è¯¯æƒé‡
            error_weights = error_matrix[target_labels, pred_classes]
            
            # åº”ç”¨æ–¹å‘æ€§é”™è¯¯æƒ©ç½š
            directional_loss = adjusted_loss * (1.0 + error_weights * 0.1)
            task_loss = directional_loss.mean()
            
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses

class MultiTaskLoss(nn.Module):
    """ä¼ ç»Ÿå¤šä»»åŠ¡æŸå¤±å‡½æ•°ï¼ˆä¿ç•™ç”¨äºå¯¹æ¯”ï¼‰"""
    
    def __init__(self, task_weights: Dict[str, float], class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.task_weights = task_weights
        self.class_weights = class_weights
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å¤šä»»åŠ¡æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            task_loss = F.cross_entropy(pred_logits, target_labels, weight=self.class_weights)
            
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses

class ProbabilityAdjustedLoss(nn.Module):
    """æ–¹æ¡ˆ1: åŸºäºåŸºç¡€æ¦‚ç‡è°ƒæ•´çš„æŸå¤±å‡½æ•°
    
    æ ¸å¿ƒæ€æƒ³ï¼šè€ƒè™‘ç±»åˆ«çš„åŸºç¡€åˆ†å¸ƒæ¦‚ç‡ï¼Œè°ƒæ•´æŸå¤±æƒé‡
    - ç¨³å®šç±»åŸºç¡€æ¦‚ç‡å¾ˆé«˜(95%)ï¼Œé¢„æµ‹ç¨³å®šä¸åº”è¯¥å¾—åˆ°å¤ªå¤šå¥–åŠ±
    - å˜åŒ–ç±»åŸºç¡€æ¦‚ç‡å¾ˆä½(5%)ï¼Œæ­£ç¡®é¢„æµ‹å˜åŒ–åº”è¯¥å¾—åˆ°æ›´å¤šå¥–åŠ±
    """
    
    def __init__(self, task_weights: Dict[str, float], 
                 base_stable_prob: float = 0.95,
                 base_change_prob: float = 0.05):
        super().__init__()
        self.task_weights = task_weights
        self.base_stable_prob = base_stable_prob
        self.base_change_prob = base_change_prob
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """åŸºç¡€æ¦‚ç‡è°ƒæ•´æŸå¤±è®¡ç®—"""
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            pred_probs = F.softmax(pred_logits, dim=1)
            
            batch_losses = []
            for i, (pred, true) in enumerate(zip(pred_probs, target_labels)):
                if true == 0:  # ç¨³å®šç±»
                    # è®¡ç®—è¶…å‡ºåŸºç¡€æ¦‚ç‡çš„éƒ¨åˆ†
                    excess_prob = max(0, pred[0].item() - self.base_stable_prob)
                    adjustment = excess_prob / (1 - self.base_stable_prob) if (1 - self.base_stable_prob) > 0 else 0
                    # è°ƒæ•´æƒé‡ï¼šé¢„æµ‹æ¦‚ç‡è¶…å‡ºåŸºç¡€æ¦‚ç‡æ—¶å‡å°‘æƒ©ç½š
                    weight = max(0.1, 1.0 - adjustment * 0.5)
                    loss = -weight * torch.log(pred[0] + 1e-8)
                else:  # å˜åŒ–ç±»
                    # å˜åŒ–ç±»é¢„æµ‹æ­£ç¡®æ—¶ç»™äºˆå¥–åŠ±
                    excess_prob = max(0, pred[1].item() - self.base_change_prob)
                    adjustment = excess_prob / (1 - self.base_change_prob) if (1 - self.base_change_prob) > 0 else 0
                    # é¢„æµ‹æ¦‚ç‡è¶…å‡ºåŸºç¡€æ¦‚ç‡æ—¶å‡å°‘æƒ©ç½šï¼Œå¦åˆ™é‡æƒ©ç½š
                    weight = max(0.5, 2.0 - adjustment)
                    loss = -weight * torch.log(pred[1] + 1e-8)
                
                batch_losses.append(loss)
            
            task_loss = torch.stack(batch_losses).mean()
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses


class ConfidenceWeightedLoss(nn.Module):
    """æ–¹æ¡ˆ2: åŸºäºé¢„æµ‹ç½®ä¿¡åº¦çš„åŠ¨æ€æƒé‡æŸå¤±å‡½æ•°
    
    æ ¸å¿ƒæ€æƒ³ï¼šæ ¹æ®æ¨¡å‹çš„é¢„æµ‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡
    - é«˜ç½®ä¿¡åº¦é¢„æµ‹é”™è¯¯ï¼šé‡æƒ©ç½š
    - é«˜ç½®ä¿¡åº¦é¢„æµ‹æ­£ç¡®ï¼šå°æƒ©ç½š  
    - ä½ç½®ä¿¡åº¦ï¼šä¸­ç­‰æƒ©ç½šï¼Œé¼“åŠ±æ¨¡å‹æé«˜ç½®ä¿¡åº¦
    """
    
    def __init__(self, task_weights: Dict[str, float],
                 confidence_threshold: float = 0.8,
                 high_conf_correct_weight: float = 0.3,
                 high_conf_wrong_weight: float = 3.0,
                 low_conf_weight: float = 1.0):
        super().__init__()
        self.task_weights = task_weights
        self.confidence_threshold = confidence_threshold
        self.high_conf_correct_weight = high_conf_correct_weight
        self.high_conf_wrong_weight = high_conf_wrong_weight
        self.low_conf_weight = low_conf_weight
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ç½®ä¿¡åº¦åŠ¨æ€æƒé‡æŸå¤±è®¡ç®—"""
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            pred_probs = F.softmax(pred_logits, dim=1)
            
            batch_losses = []
            for i, (pred, true) in enumerate(zip(pred_probs, target_labels)):
                confidence = torch.max(pred)  # æœ€é«˜æ¦‚ç‡ä½œä¸ºç½®ä¿¡åº¦
                pred_class = torch.argmax(pred)
                
                # æ ¹æ®ç½®ä¿¡åº¦å’Œæ­£ç¡®æ€§ç¡®å®šæƒé‡
                if confidence > self.confidence_threshold:
                    if pred_class == true:
                        # é«˜ç½®ä¿¡åº¦ä¸”é¢„æµ‹æ­£ç¡®ï¼šå°æƒ©ç½š
                        weight = self.high_conf_correct_weight
                    else:
                        # é«˜ç½®ä¿¡åº¦ä½†é¢„æµ‹é”™è¯¯ï¼šå¤§æƒ©ç½š
                        weight = self.high_conf_wrong_weight
                        # å˜åŒ–ç±»é¢„æµ‹é”™è¯¯æ—¶é¢å¤–æƒ©ç½š
                        if true == 1:
                            weight *= 2.0
                else:
                    # ä½ç½®ä¿¡åº¦ï¼šä¸­ç­‰æƒ©ç½š
                    weight = self.low_conf_weight
                
                loss = -weight * torch.log(pred[true] + 1e-8)
                batch_losses.append(loss)
            
            task_loss = torch.stack(batch_losses).mean()
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses


class BusinessCostLoss(nn.Module):
    """æ–¹æ¡ˆ3: åŸºäºä¸šåŠ¡æˆæœ¬çš„æŸå¤±å‡½æ•°
    
    æ ¸å¿ƒæ€æƒ³ï¼šç›´æ¥æ ¹æ®ä¸šåŠ¡åœºæ™¯ä¸­ä¸åŒé”™è¯¯çš„æˆæœ¬æ¥è®¾è®¡æŸå¤±
    - ç¨³å®šè¯¯åˆ¤ä¸ºå˜åŒ–ï¼šå¯¼è‡´ä¸å¿…è¦çš„äº¤æ˜“ï¼Œæˆæœ¬è¾ƒä½
    - å˜åŒ–è¯¯åˆ¤ä¸ºç¨³å®šï¼šé”™è¿‡äº¤æ˜“æœºä¼šï¼Œæˆæœ¬è¾ƒé«˜
    - åˆ†åˆ«è®¾å®šç¨³å®šç±»å’Œå˜åŒ–ç±»çš„é¢„æµ‹æ­£ç¡®å¥–åŠ±
    """
    
    def __init__(self, task_weights: Dict[str, float],
                 false_alarm_cost: float = 1.0,        # è¯¯æŠ¥æˆæœ¬(ç¨³å®š->å˜åŒ–)
                 miss_change_cost: float = 5.0,        # æ¼æŠ¥æˆæœ¬(å˜åŒ–->ç¨³å®š)
                 stable_correct_reward: float = 0.1,   # ç¨³å®šç±»é¢„æµ‹æ­£ç¡®å¥–åŠ±
                 change_correct_reward: float = 0.2):  # å˜åŒ–ç±»é¢„æµ‹æ­£ç¡®å¥–åŠ±
        super().__init__()
        self.task_weights = task_weights
        self.false_alarm_cost = false_alarm_cost
        self.miss_change_cost = miss_change_cost
        self.stable_correct_reward = stable_correct_reward
        self.change_correct_reward = change_correct_reward
        
        # æˆæœ¬çŸ©é˜µ: [çœŸå®æ ‡ç­¾][é¢„æµ‹æ ‡ç­¾]
        self.cost_matrix = torch.tensor([
            [0.0, false_alarm_cost],     # çœŸç¨³å®š: [é¢„æµ‹ç¨³å®š, é¢„æµ‹å˜åŒ–]
            [miss_change_cost, 0.0]      # çœŸå˜åŒ–: [é¢„æµ‹ç¨³å®š, é¢„æµ‹å˜åŒ–]
        ])
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ä¸šåŠ¡æˆæœ¬é©±åŠ¨æŸå¤±è®¡ç®—"""
        losses = {}
        total_loss = 0.0
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            pred_probs = F.softmax(pred_logits, dim=1)
            
            # ç¡®ä¿æˆæœ¬çŸ©é˜µåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            cost_matrix = self.cost_matrix.to(pred_logits.device)
            
            batch_losses = []
            for i, (pred, true) in enumerate(zip(pred_probs, target_labels)):
                pred_class = torch.argmax(pred)
                
                if true == pred_class:
                    # é¢„æµ‹æ­£ç¡®ï¼šåˆ†åˆ«å¯¹å¾…ç¨³å®šç±»å’Œå˜åŒ–ç±»
                    if true == 0:  # ç¨³å®šç±»é¢„æµ‹æ­£ç¡®
                        loss = -self.stable_correct_reward * torch.log(pred[true] + 1e-8)
                    else:  # å˜åŒ–ç±»é¢„æµ‹æ­£ç¡®
                        loss = -self.change_correct_reward * torch.log(pred[true] + 1e-8)
                else:
                    # é¢„æµ‹é”™è¯¯ï¼šæŒ‰ä¸šåŠ¡æˆæœ¬åŠ æƒ
                    cost = cost_matrix[true, pred_class]
                    # ç»“åˆäº¤å‰ç†µæŸå¤±å’Œä¸šåŠ¡æˆæœ¬
                    base_loss = -torch.log(pred[true] + 1e-8)
                    loss = cost * base_loss
                
                batch_losses.append(loss)
            
            task_loss = torch.stack(batch_losses).mean()
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses


class ImbalancedFocalLoss(nn.Module):
    """æ–¹æ¡ˆ4: é’ˆå¯¹æåº¦ä¸å¹³è¡¡æ•°æ®çš„æ”¹è¿›Focal Loss
    
    æ ¸å¿ƒæ€æƒ³ï¼šFocal Loss + ç±»åˆ«å¹³è¡¡ + åŠ¨æ€éš¾åº¦æƒé‡
    - alpha: ç±»åˆ«å¹³è¡¡å› å­ï¼Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡
    - gamma: èšç„¦å› å­ï¼Œå…³æ³¨å›°éš¾æ ·æœ¬
    - åŠ¨æ€è°ƒæ•´ï¼šæ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å‚æ•°
    """
    
    def __init__(self, task_weights: Dict[str, float],
                 alpha: float = 0.25,          # ç±»åˆ«å¹³è¡¡å› å­
                 gamma: float = 2.0,           # èšç„¦å› å­
                 dynamic_adjustment: bool = True):
        super().__init__()
        self.task_weights = task_weights
        self.alpha = alpha
        self.gamma = gamma
        self.dynamic_adjustment = dynamic_adjustment
        self.training_step = 0
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """æ”¹è¿›Focal Lossè®¡ç®—"""
        losses = {}
        total_loss = 0.0
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°ï¼ˆå¯é€‰ï¼‰
        if self.dynamic_adjustment:
            # è®­ç»ƒåˆæœŸæ›´å…³æ³¨å¹³è¡¡ï¼ŒåæœŸæ›´å…³æ³¨å›°éš¾æ ·æœ¬
            dynamic_alpha = max(0.1, self.alpha - self.training_step * 0.0001)
            dynamic_gamma = min(3.0, self.gamma + self.training_step * 0.0001)
        else:
            dynamic_alpha = self.alpha
            dynamic_gamma = self.gamma
        
        for task_name, pred_logits in predictions.items():
            if task_name not in targets:
                continue
                
            target_labels = targets[task_name]
            pred_probs = F.softmax(pred_logits, dim=1)
            
            batch_losses = []
            for i, (pred, true) in enumerate(zip(pred_probs, target_labels)):
                pt = pred[true]  # æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡
                
                # ç±»åˆ«å¹³è¡¡æƒé‡
                if true == 0:  # ç¨³å®šç±»
                    alpha_t = 1 - dynamic_alpha  # ç»™å¤šæ•°ç±»è¾ƒå°æƒé‡
                else:  # å˜åŒ–ç±»
                    alpha_t = dynamic_alpha      # ç»™å°‘æ•°ç±»è¾ƒå¤§æƒé‡
                
                # Focalæƒé‡ï¼š(1-pt)^gammaï¼Œå›°éš¾æ ·æœ¬æƒé‡å¤§
                focal_weight = torch.pow(1 - pt, dynamic_gamma)
                
                # ç»¼åˆæŸå¤±
                loss = -alpha_t * focal_weight * torch.log(pt + 1e-8)
                
                # é¢å¤–çš„å˜åŒ–ç±»æ¿€åŠ±
                if true == 1:
                    loss *= 1.5  # å˜åŒ–ç±»é¢å¤–1.5å€æƒé‡
                
                batch_losses.append(loss)
            
            task_loss = torch.stack(batch_losses).mean()
            task_weight = self.task_weights.get(task_name, 1.0)
            weighted_loss = task_weight * task_loss
            
            losses[f'loss_{task_name}'] = task_loss
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        
        # æ›´æ–°è®­ç»ƒæ­¥æ•°
        self.training_step += 1
        
        return losses

def create_model(config: Dict) -> MarketTransformer:
    """
    åˆ›å»ºæ¨¡å‹å·¥å‚å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å‹
    """
    return MarketTransformer(config)

def test_transformer_model():
    """æµ‹è¯•Transformeræ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•Transformeræ¨¡å‹...")
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'feature_dim': 38,
        'embed_dim': 256,
        'num_heads': 8,
        'num_layers': 6,
        'ff_dim': 1024,
        'dropout': 0.1,
        'sequence_length': 30,
        'prediction_horizons': [5, 10, 15]
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = MarketTransformer(config)
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 32
    seq_length = 30
    feature_dim = 38
    
    x = torch.randn(batch_size, seq_length, feature_dim)
    
    # å‰å‘ä¼ æ’­
    outputs = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print("è¾“å‡ºå½¢çŠ¶:")
    for horizon, output in outputs.items():
        print(f"  {horizon}: {output.shape}")
    
    # æµ‹è¯•æ¦‚ç‡é¢„æµ‹
    probas = model.predict_proba(x)
    print("\næ¦‚ç‡é¢„æµ‹:")
    for horizon, proba in probas.items():
        print(f"  {horizon}: {proba.shape}, sum={proba.sum(dim=1)[0]:.3f}")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\næ¨¡å‹å‚æ•°:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model

class TwoTaskTransformer(nn.Module):
    """
    ä¸¤ä»»åŠ¡Transformeræ¨¡å‹ - ç¨³å®šæ€§æ£€æµ‹ + æ–¹å‘åˆ¤æ–­
    
    ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹ (stable vs unstable)
    ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­ (up vs down, ä»…åœ¨unstableæ—¶)
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # é…ç½®å‚æ•°
        self.feature_dim = config['feature_dim']
        self.embed_dim = config['embed_dim']
        self.num_heads = config['num_heads']
        self.num_layers = config['num_layers']
        self.ff_dim = config['ff_dim']
        self.dropout = config['dropout']
        self.sequence_length = config['sequence_length']
        self.prediction_horizons = config['prediction_horizons']
        
        # è¾“å…¥åµŒå…¥å±‚
        self.input_embedding = nn.Linear(self.feature_dim, self.embed_dim)
        self.input_norm = nn.LayerNorm(self.embed_dim)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(self.embed_dim, self.sequence_length)
        
        # å…±äº«çš„Transformerå±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.dropout)
            for _ in range(self.num_layers)
        ])
        
        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹å¤´ï¼ˆæ¯ä¸ªæ—¶é—´å°ºåº¦ä¸€ä¸ªï¼‰
        self.stability_heads = nn.ModuleDict()
        for horizon in self.prediction_horizons:
            self.stability_heads[f'{horizon}min'] = nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 2, 2)  # stable=0, unstable=1
            )
        
        # ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­å¤´ï¼ˆæ¯ä¸ªæ—¶é—´å°ºåº¦ä¸€ä¸ªï¼‰
        self.direction_heads = nn.ModuleDict()
        for horizon in self.prediction_horizons:
            self.direction_heads[f'{horizon}min'] = nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout),
                nn.Linear(self.embed_dim // 2, 2)  # up=0, down=1
            )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, sequence_length, feature_dim]
            lengths: åºåˆ—å®é™…é•¿åº¦ [batch_size] (å¯é€‰)
            
        Returns:
            ä¸¤ä¸ªä»»åŠ¡çš„é¢„æµ‹ç»“æœå­—å…¸
        """
        batch_size, seq_len, feature_dim = x.size()
        
        # è¾“å…¥åµŒå…¥å’Œå½’ä¸€åŒ–
        x = self.input_embedding(x)  # [batch, seq_len, embed_dim]
        x = self.input_norm(x)
        
        # ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # [seq_len, batch, embed_dim]
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # [batch, seq_len, embed_dim]
        
        # Transformerå±‚
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        # å…¨å±€æ± åŒ–ï¼šå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        pooled_output = x[:, -1, :]  # [batch, embed_dim]
        
        # å¤šä»»åŠ¡é¢„æµ‹
        predictions = {}
        
        # ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹
        for horizon in self.prediction_horizons:
            stability_logits = self.stability_heads[f'{horizon}min'](pooled_output)
            predictions[f'stability_{horizon}min'] = stability_logits
        
        # ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­
        for horizon in self.prediction_horizons:
            direction_logits = self.direction_heads[f'{horizon}min'](pooled_output)
            predictions[f'direction_{horizon}min'] = direction_logits
        
        return predictions


class TwoTaskLoss(nn.Module):
    """ä¸¤ä»»åŠ¡æŸå¤±å‡½æ•°"""
    
    def __init__(self, task_weights: Dict[str, float], 
                 stability_weight: float = 0.4,
                 direction_weight: float = 0.6):
        super().__init__()
        self.task_weights = task_weights
        self.stability_weight = stability_weight  # ç¨³å®šæ€§ä»»åŠ¡æƒé‡
        self.direction_weight = direction_weight  # æ–¹å‘ä»»åŠ¡æƒé‡
        
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—ä¸¤ä»»åŠ¡æŸå¤±
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        # ä»»åŠ¡1ï¼šç¨³å®šæ€§æ£€æµ‹æŸå¤±
        stability_loss = 0.0
        stability_count = 0
        
        for task_name, pred_logits in predictions.items():
            if task_name.startswith('stability_'):
                horizon_key = task_name.replace('stability_', '')
                if task_name in targets:
                    target_labels = targets[task_name]
                    task_loss = F.cross_entropy(pred_logits, target_labels)
                    
                    stability_loss += task_loss * self.task_weights.get(horizon_key, 1.0)
                    stability_count += 1
                    losses[f'loss_{task_name}'] = task_loss
        
        if stability_count > 0:
            stability_loss /= stability_count
            total_loss += self.stability_weight * stability_loss
            losses['stability_loss'] = stability_loss
        
        # ä»»åŠ¡2ï¼šæ–¹å‘åˆ¤æ–­æŸå¤±ï¼ˆåªåœ¨æœ‰æ•ˆæ ·æœ¬ä¸Šè®¡ç®—ï¼‰
        direction_loss = 0.0
        direction_count = 0
        
        for task_name, pred_logits in predictions.items():
            if task_name.startswith('direction_'):
                horizon_key = task_name.replace('direction_', '')
                if task_name in targets:
                    target_labels = targets[task_name]
                    
                    # è¿‡æ»¤æ‰æ— æ•ˆæ ‡ç­¾ï¼ˆ-1ï¼‰
                    valid_mask = target_labels != -1
                    if valid_mask.sum() > 0:
                        valid_pred = pred_logits[valid_mask]
                        valid_target = target_labels[valid_mask]
                        
                        task_loss = F.cross_entropy(valid_pred, valid_target)
                        direction_loss += task_loss * self.task_weights.get(horizon_key, 1.0)
                        direction_count += 1
                        losses[f'loss_{task_name}'] = task_loss
        
        if direction_count > 0:
            direction_loss /= direction_count
            total_loss += self.direction_weight * direction_loss
            losses['direction_loss'] = direction_loss
        
        losses['total_loss'] = total_loss
        return losses

if __name__ == "__main__":
    test_transformer_model() 