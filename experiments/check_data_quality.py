#!/usr/bin/env python3
"""
ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·
æ£€æŸ¥ä¸‹è½½çš„å¸å®‰æ•°æ®çš„æ—¶é—´è¿ç»­æ€§ã€ä»·æ ¼åˆç†æ€§ç­‰è´¨é‡æŒ‡æ ‡
"""

from datetime import datetime, timedelta

import numpy as np
import pandas as pd


def check_data_quality(file_path: str):
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    df = pd.read_csv(file_path)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    print(f"æ€»è®°å½•æ•°: {len(df):,}")
    print(f"æ—¶é—´èŒƒå›´: {df.timestamp.min()} åˆ° {df.timestamp.max()}")
    print(f"å®é™…å¤©æ•°: {(df.timestamp.max() - df.timestamp.min()).days} å¤©")
    
    # æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
    print("\nğŸ•’ æ—¶é—´è¿ç»­æ€§æ£€æŸ¥:")
    df_sorted = df.sort_values('timestamp').reset_index(drop=True)
    time_diffs = df_sorted['timestamp'].diff().dropna()
    
    # æ ‡å‡†é—´éš”åº”è¯¥æ˜¯1åˆ†é’Ÿ
    expected_interval = timedelta(minutes=1)
    normal_intervals = (time_diffs == expected_interval).sum()
    total_intervals = len(time_diffs)
    
    print(f"æ ‡å‡†1åˆ†é’Ÿé—´éš”: {normal_intervals:,}/{total_intervals:,} ({normal_intervals/total_intervals*100:.2f}%)")
    
    # æ£€æŸ¥å¼‚å¸¸é—´éš”
    abnormal_intervals = time_diffs[time_diffs != expected_interval]
    if len(abnormal_intervals) > 0:
        print(f"å¼‚å¸¸é—´éš”æ•°é‡: {len(abnormal_intervals)}")
        print(f"å¼‚å¸¸é—´éš”ç»Ÿè®¡:")
        
        # æŒ‰å¼‚å¸¸ç±»å‹åˆ†ç±»
        short_gaps = abnormal_intervals[abnormal_intervals < expected_interval]
        long_gaps = abnormal_intervals[abnormal_intervals > expected_interval]
        
        if len(short_gaps) > 0:
            print(f"  å°äº1åˆ†é’Ÿçš„é—´éš”: {len(short_gaps)} ä¸ª")
        
        if len(long_gaps) > 0:
            print(f"  å¤§äº1åˆ†é’Ÿçš„é—´éš”: {len(long_gaps)} ä¸ª")
            print(f"  æœ€å¤§é—´éš”: {long_gaps.max()}")
            
            # æ‰¾å‡ºå¤§é—´éš”çš„å…·ä½“ä½ç½®
            large_gap_indices = time_diffs[time_diffs > timedelta(minutes=5)].index
            if len(large_gap_indices) > 0:
                print(f"  å¤§äº5åˆ†é’Ÿçš„é—´éš”ä½ç½®:")
                for idx in large_gap_indices[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                    before_time = df_sorted.iloc[idx-1]['timestamp']
                    after_time = df_sorted.iloc[idx]['timestamp']
                    gap = after_time - before_time
                    print(f"    {before_time} -> {after_time} (é—´éš”: {gap})")
    
    # æ£€æŸ¥é‡å¤æ—¶é—´æˆ³
    duplicates = df['timestamp'].duplicated().sum()
    print(f"é‡å¤æ—¶é—´æˆ³: {duplicates} ä¸ª")
    
    # æ£€æŸ¥é¢„æœŸçš„æ€»è®°å½•æ•°
    expected_total = (df.timestamp.max() - df.timestamp.min()).total_seconds() / 60 + 1
    coverage = len(df) / expected_total * 100
    print(f"æ—¶é—´è¦†ç›–ç‡: {coverage:.2f}% ({len(df):,}/{expected_total:.0f})")
    
    print("\nğŸ“ˆ ä»·æ ¼æ•°æ®è´¨é‡æ£€æŸ¥:")
    print(f"å¼€ç›˜ä»·èŒƒå›´: {df.open.min():.6f} - {df.open.max():.6f}")
    print(f"æ”¶ç›˜ä»·èŒƒå›´: {df.close.min():.6f} - {df.close.max():.6f}")
    print(f"æœ€é«˜ä»·èŒƒå›´: {df.high.min():.6f} - {df.high.max():.6f}")
    print(f"æœ€ä½ä»·èŒƒå›´: {df.low.min():.6f} - {df.low.max():.6f}")
    
    # ä»·æ ¼å˜åŒ–å¹…åº¦
    price_range = df.close.max() - df.close.min()
    avg_price = df.close.mean()
    price_volatility = price_range / avg_price * 100
    print(f"æ€»ä»·æ ¼å˜åŒ–å¹…åº¦: {price_volatility:.4f}%")
    
    # æ£€æŸ¥OHLCé€»è¾‘
    ohlc_errors = (
        (df.high < df.low) | 
        (df.high < df.open) | 
        (df.high < df.close) | 
        (df.low > df.open) | 
        (df.low > df.close)
    ).sum()
    print(f"OHLCé€»è¾‘é”™è¯¯: {ohlc_errors} æ¡")
    
    # æ£€æŸ¥ä»·æ ¼è·³è·ƒ
    price_changes = df['close'].pct_change().abs()
    large_changes = price_changes[price_changes > 0.001]  # 0.1%ä»¥ä¸Šå˜åŒ–
    print(f"å¤§äº0.1%çš„ä»·æ ¼è·³è·ƒ: {len(large_changes)} æ¬¡")
    if len(large_changes) > 0:
        print(f"æœ€å¤§å•æ¬¡ä»·æ ¼å˜åŒ–: {price_changes.max()*100:.4f}%")
    
    print("\nğŸ“Š æˆäº¤é‡ç»Ÿè®¡:")
    volume_stats = df.volume.describe()
    print(f"å¹³å‡æˆäº¤é‡: {volume_stats['mean']:.0f}")
    print(f"ä¸­ä½æ•°æˆäº¤é‡: {volume_stats['50%']:.0f}")
    print(f"æœ€å¤§æˆäº¤é‡: {volume_stats['max']:.0f}")
    print(f"æœ€å°æˆäº¤é‡: {volume_stats['min']:.0f}")
    
    zero_volume = (df.volume == 0).sum()
    print(f"é›¶æˆäº¤é‡è®°å½•: {zero_volume} æ¡")
    
    # å¼‚å¸¸æˆäº¤é‡æ£€æµ‹
    volume_q99 = df.volume.quantile(0.99)
    high_volume = (df.volume > volume_q99).sum()
    print(f"å¼‚å¸¸é«˜æˆäº¤é‡(>99%åˆ†ä½): {high_volume} æ¡")
    
    print("\nğŸ“… æŒ‰æœˆä»½ç»Ÿè®¡:")
    df['month'] = df['timestamp'].dt.to_period('M')
    monthly_stats = df.groupby('month').agg({
        'timestamp': 'count',
        'close': ['min', 'max', 'mean'],
        'volume': 'mean'
    }).round(6)
    
    monthly_stats.columns = ['è®°å½•æ•°', 'æœ€ä½ä»·', 'æœ€é«˜ä»·', 'å¹³å‡ä»·', 'å¹³å‡æˆäº¤é‡']
    print(monthly_stats)
    
    print("\nâœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ!")
    
    # ç”Ÿæˆè´¨é‡è¯„åˆ†
    quality_score = 0
    max_score = 100
    
    # æ—¶é—´è¿ç»­æ€§è¯„åˆ† (40åˆ†)
    continuity_score = min(40, (normal_intervals / total_intervals) * 40)
    quality_score += continuity_score
    
    # OHLCæ­£ç¡®æ€§è¯„åˆ† (20åˆ†)
    ohlc_score = 20 if ohlc_errors == 0 else max(0, 20 - ohlc_errors)
    quality_score += ohlc_score
    
    # æˆäº¤é‡åˆç†æ€§è¯„åˆ† (20åˆ†)
    volume_score = 20 if zero_volume == 0 else max(0, 20 - zero_volume)
    quality_score += volume_score
    
    # ä»·æ ¼åˆç†æ€§è¯„åˆ† (20åˆ†)
    price_score = 20 if price_volatility < 1.0 else max(0, 20 - price_volatility)
    quality_score += price_score
    
    print(f"\nğŸ¯ æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.1f}/{max_score}")
    
    if quality_score >= 90:
        print("ğŸŒŸ æ•°æ®è´¨é‡: ä¼˜ç§€")
    elif quality_score >= 80:
        print("âœ… æ•°æ®è´¨é‡: è‰¯å¥½") 
    elif quality_score >= 70:
        print("âš ï¸ æ•°æ®è´¨é‡: ä¸€èˆ¬")
    else:
        print("âŒ æ•°æ®è´¨é‡: è¾ƒå·®")
    
    return df

if __name__ == "__main__":
    file_path = "data/processed/USDCUSDT_recent_6months.csv"
    df = check_data_quality(file_path) 